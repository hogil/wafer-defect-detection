#!/usr/bin/env python3
"""
ğŸ¯ Enhanced Wafer Defect Detection - í†µí•© í›ˆë ¨+ì¶”ë¡  ì‹œìŠ¤í…œ
torchvision.datasets.ImageFolder ê¸°ë°˜ ë‹¨ìˆœí™”
"""

import os
import sys
import argparse
import json
import time
import logging
from pathlib import Path
from typing import Dict, List, Tuple, Optional, Any
import numpy as np
from PIL import Image
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, random_split
from torchvision import datasets, transforms
import timm
from ultralytics import YOLO
from sklearn.metrics import classification_report, confusion_matrix
import cv2

from roi_utils import ROIExtractor
from gradcam_utils import GradCAMAnalyzer

# ì„±ëŠ¥ ë¶„ì„ì„ ìœ„í•œ ì¶”ê°€ import
from sklearn.metrics import (
    confusion_matrix, classification_report, 
    precision_recall_fscore_support, accuracy_score
)
import matplotlib.pyplot as plt
import seaborn as sns

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
project_root = Path(__file__).parent
sys.path.append(str(project_root))

from config import ConfigManager, get_production_config, get_quick_test_config

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class WaferTrainer:
    """ğŸ¯ ì›¨ì´í¼ ë¶ˆëŸ‰ ê²€ì¶œ ì‹œìŠ¤í…œ (ì¶”ë¡  ì „ìš©)"""
    
    def __init__(self, dataset_root: str, config_manager: ConfigManager = None):
        self.dataset_root = Path(dataset_root)
        
        # ì„¤ì • ê´€ë¦¬ì
        if config_manager is None:
            self.config_manager = ConfigManager()
            self.config_manager.update_dataset_path(str(dataset_root))
        else:
            self.config_manager = config_manager
        
        self.config = self.config_manager.get_config()
        # device ì„¤ì • (í•­ìƒ GPU ì‚¬ìš©, ì—†ìœ¼ë©´ ì—ëŸ¬)
        if torch.cuda.is_available():
            self.device = torch.device('cuda')
        else:
            raise RuntimeError('CUDA(GPU)ê°€ í•„ìš”í•©ë‹ˆë‹¤. GPU í™˜ê²½ì—ì„œ ì‹¤í–‰í•˜ì„¸ìš”!')
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        self.output_dir = Path(self.config.OUTPUT_DIR)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # ëª¨ë¸ ê´€ë ¨
        self.model = None
        self.classes = []
        self.num_classes = 0
        self.class_to_idx = {}
        
        # ROI ê´€ë ¨
        self.yolo_model = None
        self.difficult_classes = []
        self.class_object_mapping = {}
        self.yolo_objects = []
        self.num_yolo_objects = 0
        
        # ROI ì¶”ì¶œê¸° ì´ˆê¸°í™” (í´ë˜ìŠ¤ë³„ íŒ¨í„´ íŒŒì¼ ì§€ì •)
        roi_patterns_file = self.output_dir / "class_roi_patterns.json"
        self.roi_extractor = ROIExtractor(str(roi_patterns_file))
        
        # Grad-CAM ë¶„ì„ê¸° (ë‚˜ì¤‘ì— ì´ˆê¸°í™”)
        self.gradcam_analyzer = None
        
        logger.info("ğŸ¯ WaferTrainer initialized (Inference Only)")
        logger.info(f"  Dataset: {self.dataset_root}")
        logger.info(f"  Output: {self.output_dir}")
        logger.info(f"  Device: {self.device}")
    
    def _create_datasets(self, train_ratio: float = 0.7):
        """ImageFolderë¡œ ë°ì´í„°ì…‹ ìƒì„±"""
        
        print("ğŸ“‚ Creating datasets with ImageFolder...")
        
        # ì „ì²´ ë°ì´í„°ì…‹ ë¡œë“œ
        full_dataset = datasets.ImageFolder(
            root=self.dataset_root,
            transform=transforms.Compose([
                transforms.Resize((self.config.CLASSIFICATION_SIZE, self.config.CLASSIFICATION_SIZE)),
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
            ])
        )
        
        # í´ë˜ìŠ¤ ì •ë³´ ì¶”ì¶œ
        self.classes = full_dataset.classes
        self.num_classes = len(self.classes)
        self.class_to_idx = full_dataset.class_to_idx
        
        print(f"âœ… Discovered {self.num_classes} classes: {self.classes}")
        
        # Train/Val ë¶„í• 
        total_size = len(full_dataset)
        train_size = int(train_ratio * total_size)
        val_size = total_size - train_size
        
        train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])
        
        # ë°ì´í„° ë¡œë” ìƒì„± (ì¶”ë¡ ìš©, augmentation ì—†ìŒ)
        self.train_loader = DataLoader(
            train_dataset, 
            batch_size=self.config.BATCH_SIZE, 
            shuffle=False,  # ì¶”ë¡ ì´ë¯€ë¡œ shuffle=False
            num_workers=4,
            pin_memory=True
        )
        
        self.val_loader = DataLoader(
            val_dataset, 
            batch_size=self.config.BATCH_SIZE * 2, 
            shuffle=False, 
            num_workers=4,
            pin_memory=True
        )
        
        print(f"ğŸ“Š Dataset split:")
        print(f"  - Train: {train_size} samples")
        print(f"  - Val: {val_size} samples")
        
        return train_dataset, val_dataset
    
    def _create_model(self):
        """ConvNeXtV2 ëª¨ë¸ ìƒì„±"""
        
        print(f"ğŸ¤– Creating ConvNeXtV2 model...")
        
        self.model = timm.create_model(
            self.config.CONVNEXT_MODEL_NAME,
            pretrained=False,  # ë³„ë„ ê°€ì¤‘ì¹˜ ë¡œë“œ
            num_classes=self.num_classes
        )
        
        # ì‚¬ì „ í›ˆë ¨ëœ ê°€ì¤‘ì¹˜ ë¡œë“œ
        pretrained_path = Path(self.config.CONVNEXT_PRETRAINED_MODEL)
        weights_loaded = False
        
        if pretrained_path.exists():
            print(f"ğŸ”„ Loading pretrained weights from: {pretrained_path}")
            pretrained_weights = torch.load(pretrained_path, map_location=self.device)
            
            # model. prefix ì œê±° (ìˆì„ ê²½ìš°)
            clean_pretrained_weights = {}
            for key, value in pretrained_weights.items():
                if key.startswith('model.'):
                    new_key = key[6:]  # "model." ì œê±°
                    clean_pretrained_weights[new_key] = value
                else:
                    clean_pretrained_weights[key] = value
            
            # ì „ì²´ ë ˆì´ì–´ strict=Trueë¡œ ë¡œë“œ (í—¤ë“œ í¬í•¨)
            self.model.load_state_dict(clean_pretrained_weights, strict=True)
            print(f"âœ… Pretrained weights loaded: {len(clean_pretrained_weights)} layers")
            weights_loaded = True
        else:
            print(f"âš ï¸ Pretrained weights not found: {pretrained_path}")
            print("   Cannot proceed without pretrained weights!")
            return False
        
        self.model.to(self.device)
        self.model.eval()  # ì¶”ë¡  ëª¨ë“œë¡œ ì„¤ì •
        
        print(f"âœ… Model created:")
        print(f"  - Architecture: {self.config.CONVNEXT_MODEL_NAME}")
        print(f"  - Classes: {self.num_classes}")
        print(f"  - Image size: {self.config.CLASSIFICATION_SIZE}")
        print(f"  - Pretrained: {'Yes' if weights_loaded else 'No'}")
        
        return True
    
    def _learn_class_roi_patterns(self):
        """Grad-CAMì„ ì‚¬ìš©í•˜ì—¬ ê° í´ë˜ìŠ¤ë³„ ROI íŒ¨í„´ í•™ìŠµ"""
        
        print("\nğŸ” Learning class-specific ROI patterns using Grad-CAM...")
        
        # Grad-CAM ë¶„ì„ê¸° ì´ˆê¸°í™”
        if self.gradcam_analyzer is None:
            # ConvNeXtV2ì˜ ë§ˆì§€ë§‰ feature layer ì§€ì •
            target_layer = "stages.3.blocks.2.norm"  # ConvNeXtV2 ë§ˆì§€ë§‰ norm layer
            self.gradcam_analyzer = GradCAMAnalyzer(self.model, target_layer_name=target_layer)
        
        # ê° í´ë˜ìŠ¤ë³„ë¡œ ROI íŒ¨í„´ ë¶„ì„
        class_roi_patterns = self.gradcam_analyzer.analyze_class_attention_patterns(
            self.val_loader, 
            self.classes, 
            num_samples_per_class=10
        )
        
        # ê° í´ë˜ìŠ¤ì˜ ëŒ€í‘œ ROI ê³„ì‚° ë° ì €ì¥
        for class_name, roi_patterns in class_roi_patterns.items():
            if roi_patterns:
                # ì¤‘ê°„ê°’(median) ë°©ì‹ìœ¼ë¡œ ëŒ€í‘œ ROI ê³„ì‚°
                representative_roi = self.gradcam_analyzer.get_representative_roi_for_class(
                    roi_patterns, method='median'
                )
                
                # ìƒëŒ€ ì¢Œí‘œë¡œ ë³€í™˜ (classification ì´ë¯¸ì§€ í¬ê¸° ê¸°ì¤€)
                classification_size = getattr(self.config, 'CLASSIFICATION_SIZE', 384)
                x1_ratio = representative_roi[0] / classification_size
                y1_ratio = representative_roi[1] / classification_size
                x2_ratio = representative_roi[2] / classification_size
                y2_ratio = representative_roi[3] / classification_size
                
                # ROI íŒ¨í„´ ì €ì¥
                self.roi_extractor.set_class_roi_pattern(
                    class_name, (x1_ratio, y1_ratio, x2_ratio, y2_ratio)
                )
                
                print(f"ğŸ“ Learned ROI for '{class_name}': ({x1_ratio:.3f},{y1_ratio:.3f}) to ({x2_ratio:.3f},{y2_ratio:.3f})")
        
        # í•™ìŠµëœ íŒ¨í„´ì„ íŒŒì¼ì— ì €ì¥
        roi_patterns_file = self.output_dir / "class_roi_patterns.json"
        self.roi_extractor.save_class_roi_patterns(str(roi_patterns_file))
        
        print(f"âœ… ROI patterns saved to {roi_patterns_file}")
    
    def _extract_roi_with_learned_pattern(self, original_image_path: str, predicted_class: str) -> np.ndarray:
        """í•™ìŠµëœ í´ë˜ìŠ¤ë³„ ROI íŒ¨í„´ì„ ì‚¬ìš©í•˜ì—¬ ROI ì¶”ì¶œ"""
        yolo_size = getattr(self.config, 'YOLO_INPUT_SIZE', 1024)
        
        return self.roi_extractor.crop_roi_from_original(
            original_image_path, 
            predicted_class,
            target_size=yolo_size
        )
    
    def _load_yolo_model(self):
        """YOLO ëª¨ë¸ ë¡œë“œ ë° ê°ì²´ ì¢…ë¥˜ ì¶”ì¶œ"""
        
        try:
            self.yolo_model = YOLO(self.config.DETECTION_MODEL)
            
            if hasattr(self.yolo_model, 'names'):
                self.yolo_objects = list(self.yolo_model.names.values())
                self.num_yolo_objects = len(self.yolo_objects)
                print(f"ğŸ¯ YOLO model loaded: {self.config.DETECTION_MODEL}")
                print(f"  Objects: {self.num_yolo_objects} classes")
                print(f"  Examples: {self.yolo_objects[:10]}...")  # ì²˜ìŒ 10ê°œë§Œ í‘œì‹œ
            else:
                print("âš ï¸ Could not extract YOLO object names")
                self.yolo_objects = []
                self.num_yolo_objects = 0
                
        except Exception as e:
            print(f"âš ï¸ YOLO model loading failed: {e}")
            self.yolo_model = None
            self.yolo_objects = []
            self.num_yolo_objects = 0
    
    def run_inference_pipeline(self):
        """ğŸ¯ ì¶”ë¡  íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (í•™ìŠµ ì—†ìŒ)"""
        
        print("\nğŸ¯ Enhanced Wafer Defect Detection - Inference Pipeline")
        print("=" * 60)
        
        # 1. ëª¨ë¸ ìƒì„± ë° ê°€ì¤‘ì¹˜ ë¡œë“œ
        if not self._create_model():
            return
        
        # 2. ë°ì´í„°ì…‹ ìƒì„±
        self._create_datasets()
        
        # 3. Classification ì „ìˆ˜ ì‹¤í–‰ ë° ì„±ëŠ¥ ë¶„ì„
        print("\nğŸ“Š STAGE 1: Classification Only Performance Analysis")
        print("-" * 50)
        
        train_cls_metrics = self._evaluate_dataset(self.train_loader, "Train-ClassificationOnly")
        val_cls_metrics = self._evaluate_dataset(self.val_loader, "Validation-ClassificationOnly")
        
        # 4. ì–´ë ¤ìš´ í´ë˜ìŠ¤ ì‹ë³„
        self._identify_difficult_classes(val_cls_metrics)
        
        # 5. Grad-CAMìœ¼ë¡œ attention map í•™ìŠµ
        print("\nğŸ§  STAGE 2: Grad-CAM Attention Pattern Learning")
        print("-" * 50)
        self._learn_class_roi_patterns()
        
        # 6. YOLO ëª¨ë¸ ë¡œë“œ
        self._load_yolo_model()
        
        # 7. ROI ë§¤í•‘ ìƒì„±
        print("\nğŸ”— STAGE 3: ROI Object Mapping Creation")
        print("-" * 50)
        self._create_roi_mappings()
        
        # 8. ROI Enhanced ì„±ëŠ¥ ë¶„ì„
        print("\nğŸ“Š STAGE 4: ROI Enhanced Performance Analysis")
        print("-" * 50)
        
        train_roi_metrics = self._evaluate_dataset_with_roi(self.train_loader, "Train-ROIEnhanced")
        val_roi_metrics = self._evaluate_dataset_with_roi(self.val_loader, "Validation-ROIEnhanced")
        
        # 9. ì„±ëŠ¥ ë¹„êµ ë° ë¦¬í¬íŠ¸ ì €ì¥
        print("\nğŸ“‹ STAGE 5: Performance Comparison & Report")
        print("-" * 50)
        
        self._save_comprehensive_performance_report(
            train_cls_metrics, val_cls_metrics,
            train_roi_metrics, val_roi_metrics
        )
        
        # 10. í´ë˜ìŠ¤ ì •ë³´ ì €ì¥
        self._save_class_info()
        
        print("\nğŸ‰ Inference Pipeline Completed!")
        print("=" * 60)
        print(f"ğŸ“ Results saved to: {self.output_dir}")
        print(f"ğŸ“Š Performance report: {self.output_dir}/performance_report.json")
        print(f"ğŸ§  ROI patterns: {self.output_dir}/class_roi_patterns.json")
        print(f"ğŸ”— Object mappings: {self.output_dir}/discovered_mappings.json")
    
    def _save_class_info(self):
        """í´ë˜ìŠ¤ ì •ë³´ ì €ì¥"""
        
        class_info = {
            'classes': self.classes,
            'num_classes': self.num_classes,
            'class_to_idx': self.class_to_idx,
            'config': {
                'CLASSIFICATION_SIZE': self.config.CLASSIFICATION_SIZE,
                'CONVNEXT_MODEL_NAME': self.config.CONVNEXT_MODEL_NAME,
                'F1_THRESHOLD': self.config.F1_THRESHOLD
            }
        }
        
        info_path = self.output_dir / 'class_info.json'
        with open(info_path, 'w', encoding='utf-8') as f:
            json.dump(class_info, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ’¾ Class info saved: {info_path}")
    
    def _identify_difficult_classes(self, val_metrics: Dict[str, Any]):
        """ì–´ë ¤ìš´ í´ë˜ìŠ¤ ì‹ë³„"""
        
        print("\nğŸ¯ Identifying difficult classes...")
        
        f1_scores = val_metrics.get('class_f1_scores', [])
        self.difficult_classes = []
        
        for i, f1 in enumerate(f1_scores):
            if f1 < self.config.F1_THRESHOLD:
                class_name = self.classes[i]
                self.difficult_classes.append(class_name)
                print(f"  âš ï¸ Difficult class: {class_name} (F1 = {f1:.3f})")
        
        print(f"âœ… Found {len(self.difficult_classes)} difficult classes")
    
    def _analyze_validation_performance(self):
        """2ë‹¨ê³„ ì„±ëŠ¥ ë¶„ì„: Classification Only â†’ ROI Enhanced"""
        
        print("ğŸ“Š Comprehensive Performance Analysis (2-Stage)")
        print("=" * 60)
        
        # === 1ë‹¨ê³„: Classification Only ì„±ëŠ¥ ===
        print("\nğŸ¤– STAGE 1: Classification Only Performance")
        print("-" * 50)
        
        train_metrics_cls = self._evaluate_dataset(self.train_loader, "Train-ClassificationOnly")
        val_metrics_cls = self._evaluate_dataset(self.val_loader, "Validation-ClassificationOnly")
        
        # ì–´ë ¤ìš´ í´ë˜ìŠ¤ ì‹ë³„ (Validation F1 ê¸°ì¤€)
        self.difficult_classes = []
        for i, f1 in enumerate(val_metrics_cls['f1_scores']):
            if f1 < self.config.F1_THRESHOLD:
                self.difficult_classes.append(self.classes[i])
        
        print(f"\nğŸ¯ Difficult classes identified: {len(self.difficult_classes)}")
        for class_name in self.difficult_classes:
            idx = self.classes.index(class_name)
            print(f"  - {class_name}: F1 = {val_metrics_cls['f1_scores'][idx]:.3f}")
        
        # === 2ë‹¨ê³„: ROI Enhanced ì„±ëŠ¥ (YOLO ë¡œë“œ í›„) ===
        print(f"\nğŸ” STAGE 2: ROI Enhanced Performance")
        print("-" * 50)
        
        # í´ë˜ìŠ¤ë³„ ROI íŒ¨í„´ í•™ìŠµ (Grad-CAM ê¸°ë°˜)
        print(f"\nğŸ§  Learning class-specific ROI patterns...")
        self._learn_class_roi_patterns()
        
        # YOLO ëª¨ë¸ ë¡œë“œ ë° ROI ë§¤í•‘ ìƒì„±
        self._load_yolo_model()
        self._create_roi_mappings()
        
        if len(self.class_object_mapping) > 0:
            print(f"âœ… ROI mappings found: {len(self.class_object_mapping)} classes")
            
            # ROI ì ìš©ëœ ì„±ëŠ¥ ë¶„ì„
            train_metrics_roi = self._evaluate_dataset_with_roi(self.train_loader, "Train-ROIEnhanced")
            val_metrics_roi = self._evaluate_dataset_with_roi(self.val_loader, "Validation-ROIEnhanced")
            
            # ì„±ëŠ¥ í–¥ìƒ ë¹„êµ
            self._compare_performance(val_metrics_cls, val_metrics_roi)
            
            # í‹€ë¦° ì˜ˆì¸¡ ìƒì„¸ ë¶„ì„ ë° ì €ì¥
            self._analyze_and_save_incorrect_predictions(val_metrics_roi)
            
            # ì „ì²´ ì„±ëŠ¥ ë¦¬í¬íŠ¸ ì €ì¥ (Both stages)
            self._save_comprehensive_performance_report(
                train_metrics_cls, val_metrics_cls,
                train_metrics_roi, val_metrics_roi
            )
        else:
            print("âš ï¸ No ROI mappings created - using Classification only")
            self._save_performance_report(train_metrics_cls, val_metrics_cls)
    
    def _evaluate_dataset(self, dataloader: DataLoader, split_name: str) -> Dict[str, Any]:
        """ë°ì´í„°ì…‹ ì„±ëŠ¥ í‰ê°€ ë° ì‹œê°í™”"""
        
        print(f"\nğŸ“ˆ Evaluating {split_name} Dataset...")
        
        self.model.eval()
        all_preds = []
        all_labels = []
        
        with torch.no_grad():
            for images, labels in dataloader:
                images, labels = images.to(self.device), labels.to(self.device)
                outputs = self.model(images)
                _, predicted = outputs.max(1)
                
                all_preds.extend(predicted.cpu().numpy())
                all_labels.extend(labels.cpu().numpy())
        
        # ë©”íŠ¸ë¦­ ê³„ì‚°
        accuracy = accuracy_score(all_labels, all_preds)
        precision, recall, f1_scores, support = precision_recall_fscore_support(
            all_labels, all_preds, average=None, zero_division=0
        )
        
        # Weighted averages ê³„ì‚°
        weighted_precision = np.average(precision, weights=support)
        weighted_recall = np.average(recall, weights=support)
        weighted_f1 = np.average(f1_scores, weights=support)
        
        # í´ë˜ìŠ¤ë³„ ì •ë‹µ/ì „ì²´ ê°œìˆ˜ ê³„ì‚°
        class_corrects = []
        class_totals = []
        for i in range(len(self.classes)):
            class_mask = np.array(all_labels) == i
            class_total = np.sum(class_mask)
            class_correct = np.sum((np.array(all_preds)[class_mask]) == i)
            class_corrects.append(class_correct)
            class_totals.append(class_total)
        
        # Confusion Matrix ìƒì„± ë° ì €ì¥
        cm = confusion_matrix(all_labels, all_preds)
        self._plot_confusion_matrix(cm, split_name)
        
        # í´ë˜ìŠ¤ë³„ ìƒì„¸ ë¦¬í¬íŠ¸ ì¶œë ¥
        print(f"\nğŸ“Š {split_name} Performance Summary:")
        print(f"  Overall Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)")
        print(f"  Weighted F1-Score: {weighted_f1:.4f}")
        
        print(f"\nğŸ“‹ {split_name} Class-wise Metrics:")
        print("-" * 90)
        print(f"{'Class':<20} {'Precision':<10} {'Recall':<10} {'F1-Score':<10} {'Correct':<8} {'Total':<8} {'Support':<8}")
        print("-" * 90)
        
        for i, class_name in enumerate(self.classes):
            # ë°°ì—´ í¬ê¸° ì•ˆì „ ì²´í¬
            prec = precision[i] if i < len(precision) else 0.0
            rec = recall[i] if i < len(recall) else 0.0
            f1 = f1_scores[i] if i < len(f1_scores) else 0.0
            sup = support[i] if i < len(support) else 0
            
            print(f"{class_name:<20} {prec:<10.3f} {rec:<10.3f} "
                  f"{f1:<10.3f} {class_corrects[i]:<8} {class_totals[i]:<8} {sup:<8}")
        
        print("-" * 90)
        print(f"{'Weighted Avg':<20} {weighted_precision:<10.3f} {weighted_recall:<10.3f} "
              f"{weighted_f1:<10.3f} {sum(class_corrects):<8} {sum(class_totals):<8} {np.sum(support):<8}")
        
        return {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1_scores': f1_scores,
            'support': support,
            'weighted_precision': weighted_precision,
            'weighted_recall': weighted_recall,
            'weighted_f1': weighted_f1,
            'class_corrects': class_corrects,
            'class_totals': class_totals,
            'confusion_matrix': cm.tolist(),
            'predictions': all_preds,
            'true_labels': all_labels
        }
    
    def _evaluate_dataset_with_roi(self, dataloader: DataLoader, split_name: str) -> Dict[str, Any]:
        """ROIë¥¼ ì ìš©í•œ ë°ì´í„°ì…‹ ì„±ëŠ¥ í‰ê°€"""
        
        print(f"\nğŸ“ˆ Evaluating {split_name} Dataset (with ROI)...")
        
        self.model.eval()
        all_preds_initial = []  # Classificationë§Œì˜ ì˜ˆì¸¡
        all_preds_final = []    # ROI ì ìš© í›„ ìµœì¢… ì˜ˆì¸¡
        all_labels = []
        roi_usage_stats = {'used': 0, 'success': 0, 'total': 0}
        detailed_predictions = []  # ìƒì„¸ ì˜ˆì¸¡ ì •ë³´
        
        with torch.no_grad():
            for batch_idx, (images, labels) in enumerate(dataloader):
                images, labels = images.to(self.device), labels.to(self.device)
                
                for i in range(len(images)):
                    # 1. Classification ì˜ˆì¸¡
                    single_image = images[i].unsqueeze(0)
                    outputs = self.model(single_image)
                    probabilities = torch.softmax(outputs, dim=1).cpu().numpy()[0]
                    initial_pred_idx = np.argmax(probabilities)
                    initial_confidence = float(probabilities[initial_pred_idx])
                    initial_pred_class = self.classes[initial_pred_idx]
                    
                    true_label_idx = labels[i].item()
                    true_class = self.classes[true_label_idx]
                    
                    # 2. ROI í´ë˜ìŠ¤ ë³€ê²½ ì ìš©
                    final_pred_class = initial_pred_class
                    final_pred_idx = initial_pred_idx
                    detected_object = None
                    roi_used = False
                    roi_success = False
                    
                    # ROI ì¡°ê±´ í™•ì¸
                    needs_roi = (
                        initial_confidence < self.config.CONFIDENCE_THRESHOLD and
                        len(self.class_object_mapping) > 0
                    )
                    
                    if needs_roi:
                        roi_used = True
                        roi_usage_stats['used'] += 1
                        
                        # ì´ë¯¸ì§€ë¥¼ PILë¡œ ë³€í™˜í•˜ì—¬ ROI ì˜ì—­ë§Œ YOLO ì‹¤í–‰
                        image_np = single_image[0].permute(1, 2, 0).cpu().numpy()
                        image_np = image_np * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])
                        image_np = np.clip(image_np * 255, 0, 255).astype(np.uint8)
                        
                        try:
                            # ROI ì˜ì—­ë§Œ ì¶”ì¶œí•´ì„œ YOLO ì‹¤í–‰
                            roi_image = self._extract_roi_region(image_np)
                            results = self.yolo_model(roi_image, verbose=False)
                            if len(results) > 0 and len(results[0].boxes) > 0:
                                confidences = results[0].boxes.conf.cpu().numpy()
                                classes = results[0].boxes.cls.cpu().numpy()
                                
                                valid_indices = confidences > self.config.OBJECT_CONFIDENCE_THRESHOLD
                                if np.any(valid_indices):
                                    # ì‹ ë¢°ë„ ê¸°ì¤€ í•„í„°ë§ëœ ê°ì²´ë“¤ ì¤‘ì—ì„œ ê°œìˆ˜ê°€ ê°€ì¥ ë§ì€ ì¢…ë¥˜ ì„ íƒ
                                    valid_classes = classes[valid_indices]
                                    
                                    # ê° ê°ì²´ í´ë˜ìŠ¤ë³„ ê°œìˆ˜ ì„¸ê¸°
                                    unique_classes, counts = np.unique(valid_classes, return_counts=True)
                                    
                                    # ê°€ì¥ ë§ì´ ê²€ì¶œëœ ê°ì²´ í´ë˜ìŠ¤ ì„ íƒ
                                    most_frequent_idx = np.argmax(counts)
                                    detected_class_idx = int(unique_classes[most_frequent_idx])
                                    object_count = counts[most_frequent_idx]
                                    
                                    if detected_class_idx < len(self.yolo_objects):
                                        detected_object = self.yolo_objects[detected_class_idx]
                                        print(f"ğŸ¯ Most frequent object: '{detected_object}' (count: {object_count})")
                                        
                                        # ì—­ë§¤í•‘ìœ¼ë¡œ í´ë˜ìŠ¤ ì°¾ê¸°
                                        for class_name, mapped_object in self.class_object_mapping.items():
                                            if mapped_object == detected_object:
                                                final_pred_class = class_name
                                                final_pred_idx = self.classes.index(class_name)
                                                roi_success = True
                                                roi_usage_stats['success'] += 1
                                                break
                        except Exception:
                            pass
                    
                    # ê²°ê³¼ ì €ì¥
                    all_preds_initial.append(initial_pred_idx)
                    all_preds_final.append(final_pred_idx)
                    all_labels.append(true_label_idx)
                    roi_usage_stats['total'] += 1
                    
                    # ìƒì„¸ ì •ë³´ ì €ì¥
                    detailed_predictions.append({
                        'batch_idx': batch_idx,
                        'image_idx': i,
                        'true_class': true_class,
                        'initial_pred_class': initial_pred_class,
                        'initial_confidence': initial_confidence,
                        'detected_object': detected_object,
                        'final_pred_class': final_pred_class,
                        'roi_used': roi_used,
                        'roi_success': roi_success,
                        'correct_initial': (initial_pred_idx == true_label_idx),
                        'correct_final': (final_pred_idx == true_label_idx)
                    })
        
        # ìµœì¢… ì˜ˆì¸¡ ê¸°ì¤€ìœ¼ë¡œ ë©”íŠ¸ë¦­ ê³„ì‚°
        accuracy = accuracy_score(all_labels, all_preds_final)
        precision, recall, f1_scores, support = precision_recall_fscore_support(
            all_labels, all_preds_final, average=None, zero_division=0
        )
        
        # Weighted averages ê³„ì‚°
        weighted_precision = np.average(precision, weights=support)
        weighted_recall = np.average(recall, weights=support)
        weighted_f1 = np.average(f1_scores, weights=support)
        
        # í´ë˜ìŠ¤ë³„ ì •ë‹µ/ì „ì²´ ê°œìˆ˜ ê³„ì‚°
        class_corrects = []
        class_totals = []
        for i in range(len(self.classes)):
            class_mask = np.array(all_labels) == i
            class_total = np.sum(class_mask)
            class_correct = np.sum((np.array(all_preds_final)[class_mask]) == i)
            class_corrects.append(class_correct)
            class_totals.append(class_total)
        
        # Confusion Matrix ìƒì„± ë° ì €ì¥
        cm = confusion_matrix(all_labels, all_preds_final)
        self._plot_confusion_matrix(cm, split_name)
        
        # ROI í†µê³„ ì¶œë ¥
        print(f"\nğŸ“Š {split_name} Performance Summary:")
        print(f"  Overall Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)")
        print(f"  Weighted F1-Score: {weighted_f1:.4f}")
        print(f"  ROI Usage: {roi_usage_stats['used']}/{roi_usage_stats['total']} ({roi_usage_stats['used']/roi_usage_stats['total']*100:.1f}%)")
        print(f"  ROI Success: {roi_usage_stats['success']}/{roi_usage_stats['used']} ({roi_usage_stats['success']/roi_usage_stats['used']*100:.1f}% of used)" if roi_usage_stats['used'] > 0 else "  ROI Success: 0%")
        
        # í´ë˜ìŠ¤ë³„ ìƒì„¸ ë¦¬í¬íŠ¸ ì¶œë ¥
        print(f"\nğŸ“‹ {split_name} Class-wise Metrics:")
        print("-" * 90)
        print(f"{'Class':<20} {'Precision':<10} {'Recall':<10} {'F1-Score':<10} {'Correct':<8} {'Total':<8} {'Support':<8}")
        print("-" * 90)
        
        for i, class_name in enumerate(self.classes):
            # ë°°ì—´ í¬ê¸° ì•ˆì „ ì²´í¬
            prec = precision[i] if i < len(precision) else 0.0
            rec = recall[i] if i < len(recall) else 0.0
            f1 = f1_scores[i] if i < len(f1_scores) else 0.0
            sup = support[i] if i < len(support) else 0
            
            print(f"{class_name:<20} {prec:<10.3f} {rec:<10.3f} "
                  f"{f1:<10.3f} {class_corrects[i]:<8} {class_totals[i]:<8} {sup:<8}")
        
        print("-" * 90)
        print(f"{'Weighted Avg':<20} {weighted_precision:<10.3f} {weighted_recall:<10.3f} "
              f"{weighted_f1:<10.3f} {sum(class_corrects):<8} {sum(class_totals):<8} {np.sum(support):<8}")
        
        return {
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1_scores': f1_scores,
            'support': support,
            'weighted_precision': weighted_precision,
            'weighted_recall': weighted_recall,
            'weighted_f1': weighted_f1,
            'class_corrects': class_corrects,
            'class_totals': class_totals,
            'confusion_matrix': cm.tolist(),
            'predictions_initial': all_preds_initial,
            'predictions_final': all_preds_final,
            'true_labels': all_labels,
            'roi_usage_stats': roi_usage_stats,
            'detailed_predictions': detailed_predictions
        }
    
    def _plot_confusion_matrix(self, cm: np.ndarray, split_name: str):
        """Confusion Matrix ì‹œê°í™” ë° ì €ì¥"""
        
        plt.figure(figsize=(12, 10))
        
        # ì •ê·œí™”ëœ confusion matrix ê³„ì‚°
        cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        
        # Heatmap ìƒì„±
        sns.heatmap(
            cm_normalized, 
            annot=True, 
            fmt='.2f', 
            cmap='Blues',
            xticklabels=self.classes,
            yticklabels=self.classes,
            cbar_kws={'label': 'Normalized Accuracy'}
        )
        
        plt.title(f'{split_name} Confusion Matrix (Normalized)', fontsize=16, fontweight='bold')
        plt.xlabel('Predicted Class', fontsize=12)
        plt.ylabel('True Class', fontsize=12)
        plt.xticks(rotation=45, ha='right')
        plt.yticks(rotation=0)
        plt.tight_layout()
        
        # ì €ì¥
        save_path = self.output_dir / f'{split_name.lower()}_confusion_matrix.png'
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"  ğŸ“Š Confusion matrix saved: {save_path}")
    
    def _compare_performance(self, cls_metrics: Dict, roi_metrics: Dict):
        """Classification Only vs ROI Enhanced ì„±ëŠ¥ ë¹„êµ"""
        
        print(f"\nğŸ“ˆ Performance Comparison")
        print("=" * 60)
        
        cls_acc = cls_metrics['accuracy']
        roi_acc = roi_metrics['accuracy']
        cls_f1 = cls_metrics['weighted_f1']
        roi_f1 = roi_metrics['weighted_f1']
        
        acc_improvement = roi_acc - cls_acc
        f1_improvement = roi_f1 - cls_f1
        
        print(f"ğŸ“Š Overall Performance:")
        print(f"  Classification Only Accuracy:  {cls_acc:.4f} ({cls_acc*100:.2f}%)")
        print(f"  ROI Enhanced Accuracy:         {roi_acc:.4f} ({roi_acc*100:.2f}%)")
        print(f"  Accuracy Improvement:          {acc_improvement:+.4f} ({acc_improvement*100:+.2f}%)")
        
        print(f"\nğŸ“Š Weighted F1-Score:")
        print(f"  Classification Only F1:        {cls_f1:.4f}")
        print(f"  ROI Enhanced F1:               {roi_f1:.4f}")
        print(f"  F1 Improvement:                {f1_improvement:+.4f}")
        
        # í´ë˜ìŠ¤ë³„ ê°œì„  íš¨ê³¼
        print(f"\nğŸ“‹ Class-wise Improvements:")
        print("-" * 70)
        print(f"{'Class':<20} {'Cls-Only F1':<12} {'ROI-Enh F1':<12} {'Improvement':<12}")
        print("-" * 70)
        
        for i, class_name in enumerate(self.classes):
            cls_f1_class = cls_metrics['f1_scores'][i]
            roi_f1_class = roi_metrics['f1_scores'][i]
            improvement = roi_f1_class - cls_f1_class
            
            print(f"{class_name:<20} {cls_f1_class:<12.3f} {roi_f1_class:<12.3f} {improvement:+12.3f}")
        
        print("-" * 70)
        
        # ROI ì‚¬ìš© í†µê³„
        roi_stats = roi_metrics['roi_usage_stats']
        print(f"\nğŸ” ROI Usage Statistics:")
        print(f"  Total Images:       {roi_stats['total']}")
        print(f"  ROI Used:           {roi_stats['used']} ({roi_stats['used']/roi_stats['total']*100:.1f}%)")
        print(f"  ROI Success:        {roi_stats['success']} ({roi_stats['success']/roi_stats['used']*100:.1f}% of used)" if roi_stats['used'] > 0 else "  ROI Success:        0%")
        
        # ê°œì„  íš¨ê³¼ ìš”ì•½
        if acc_improvement > 0:
            print(f"\nâœ… ROI Enhancement Result: +{acc_improvement*100:.2f}% accuracy improvement!")
        elif acc_improvement == 0:
            print(f"\nâ– ROI Enhancement Result: No accuracy change")
        else:
            print(f"\nâŒ ROI Enhancement Result: {acc_improvement*100:.2f}% accuracy decrease")
    
    def _analyze_and_save_incorrect_predictions(self, roi_metrics: Dict):
        """í‹€ë¦° ì˜ˆì¸¡ë“¤ ìƒì„¸ ë¶„ì„ ë° ì´ë¯¸ì§€ ì €ì¥"""
        
        print(f"\nğŸ” Analyzing Incorrect Predictions...")
        
        # í‹€ë¦° ì˜ˆì¸¡ë“¤ë§Œ í•„í„°ë§
        detailed_preds = roi_metrics['detailed_predictions']
        incorrect_preds = [pred for pred in detailed_preds if not pred['correct_final']]
        
        if len(incorrect_preds) == 0:
            print("ğŸ‰ Perfect accuracy! No incorrect predictions to analyze.")
            return
        
        print(f"ğŸ“Š Found {len(incorrect_preds)} incorrect predictions out of {len(detailed_preds)} total")
        
        # ì˜¤ë¥˜ ë¶„ì„ ë””ë ‰í† ë¦¬ ìƒì„±
        error_analysis_dir = self.output_dir / "error_analysis"
        error_analysis_dir.mkdir(exist_ok=True)
        
        # í´ë˜ìŠ¤ë³„ ì˜¤ë¥˜ ë””ë ‰í† ë¦¬ ìƒì„±
        for class_name in self.classes:
            class_dir = error_analysis_dir / f"true_{class_name}"
            class_dir.mkdir(exist_ok=True)
        
        # ì˜¤ë¥˜ ë¶„ì„ ë°ì´í„°
        error_analysis = {
            'total_errors': len(incorrect_preds),
            'total_samples': len(detailed_preds),
            'error_rate': len(incorrect_preds) / len(detailed_preds),
            'class_wise_errors': {},
            'roi_impact_analysis': {
                'classification_only_errors': 0,
                'roi_corrected_errors': 0,
                'roi_caused_errors': 0,
                'roi_unchanged_errors': 0
            },
            'error_details': []
        }
        
        # í´ë˜ìŠ¤ë³„ ì˜¤ë¥˜ ì´ˆê¸°í™”
        for class_name in self.classes:
            error_analysis['class_wise_errors'][class_name] = {
                'total_samples': 0,
                'errors': 0,
                'error_rate': 0.0,
                'common_wrong_predictions': {}
            }
        
        print(f"ğŸ’¾ Saving error analysis images and metadata...")
        
        # ê° í‹€ë¦° ì˜ˆì¸¡ì— ëŒ€í•´ ë¶„ì„
        for idx, pred_info in enumerate(incorrect_preds):
            try:
                # ìƒì„¸ ì •ë³´ ìƒì„±
                true_class = pred_info['true_class']
                initial_pred = pred_info['initial_pred_class']
                final_pred = pred_info['final_pred_class']
                detected_obj = pred_info['detected_object'] or "None"
                
                filename = f"error_{idx:04d}_true-{true_class}_initial-{initial_pred}_final-{final_pred}_obj-{detected_obj}.json"
                
                # ìƒì„¸ ì •ë³´ë¥¼ JSONìœ¼ë¡œ ì €ì¥
                error_detail = {
                    'error_id': idx,
                    'true_class': true_class,
                    'initial_prediction': {
                        'class': initial_pred,
                        'confidence': pred_info['initial_confidence']
                    },
                    'detected_object': detected_obj,
                    'final_prediction': {
                        'class': final_pred,
                        'roi_used': pred_info['roi_used'],
                        'roi_success': pred_info['roi_success']
                    },
                    'analysis': {
                        'correct_initial': pred_info['correct_initial'],
                        'correct_final': pred_info['correct_final'],
                        'roi_impact': self._analyze_roi_impact(pred_info)
                    }
                }
                
                # JSON íŒŒì¼ë¡œ ì €ì¥
                json_path = error_analysis_dir / f"true_{true_class}" / filename
                with open(json_path, 'w', encoding='utf-8') as f:
                    json.dump(error_detail, f, indent=2, ensure_ascii=False)
                
                # ì˜¤ë¥˜ ë¶„ì„ ì—…ë°ì´íŠ¸
                error_analysis['error_details'].append(error_detail)
                
                # í´ë˜ìŠ¤ë³„ í†µê³„ ì—…ë°ì´íŠ¸
                class_stats = error_analysis['class_wise_errors'][true_class]
                class_stats['errors'] += 1
                
                if final_pred not in class_stats['common_wrong_predictions']:
                    class_stats['common_wrong_predictions'][final_pred] = 0
                class_stats['common_wrong_predictions'][final_pred] += 1
                
                # ROI ì˜í–¥ ë¶„ì„
                roi_impact = error_detail['analysis']['roi_impact']
                error_analysis['roi_impact_analysis'][roi_impact] += 1
                
            except Exception as e:
                print(f"âš ï¸ Error processing prediction {idx}: {e}")
        
        # ì „ì²´ ìƒ˜í”Œì— ëŒ€í•œ í´ë˜ìŠ¤ë³„ í†µê³„ ì™„ì„±
        for pred_info in detailed_preds:
            true_class = pred_info['true_class']
            error_analysis['class_wise_errors'][true_class]['total_samples'] += 1
        
        # ì˜¤ë¥˜ìœ¨ ê³„ì‚°
        for class_name in self.classes:
            class_stats = error_analysis['class_wise_errors'][class_name]
            if class_stats['total_samples'] > 0:
                class_stats['error_rate'] = class_stats['errors'] / class_stats['total_samples']
        
        # ì¢…í•© ì˜¤ë¥˜ ë¶„ì„ ì €ì¥
        analysis_summary_path = error_analysis_dir / "error_analysis_summary.json"
        with open(analysis_summary_path, 'w', encoding='utf-8') as f:
            json.dump(error_analysis, f, indent=2, ensure_ascii=False)
        
        # ì˜¤ë¥˜ ë¶„ì„ ê²°ê³¼ ì¶œë ¥
        print(f"\nğŸ“‹ Error Analysis Summary:")
        print(f"  Total Errors: {error_analysis['total_errors']}/{error_analysis['total_samples']} ({error_analysis['error_rate']*100:.2f}%)")
        
        print(f"\nğŸ“Š ROI Impact on Errors:")
        roi_impact = error_analysis['roi_impact_analysis']
        print(f"  Classification Only Errors: {roi_impact['classification_only_errors']}")
        print(f"  ROI Corrected Errors:       {roi_impact['roi_corrected_errors']}")
        print(f"  ROI Caused Errors:          {roi_impact['roi_caused_errors']}")
        print(f"  ROI Unchanged Errors:       {roi_impact['roi_unchanged_errors']}")
        
        print(f"\nğŸ“ Error analysis saved to: {error_analysis_dir}")
        print(f"   - Individual error JSONs: {len(incorrect_preds)} files")
        print(f"   - Summary: error_analysis_summary.json")
    
    def _analyze_roi_impact(self, pred_info: Dict) -> str:
        """ROIì˜ ì˜í–¥ ë¶„ì„"""
        
        correct_initial = pred_info['correct_initial']
        correct_final = pred_info['correct_final']
        roi_used = pred_info['roi_used']
        
        if not roi_used:
            return 'classification_only_errors'
        elif correct_initial and not correct_final:
            return 'roi_caused_errors'
        elif not correct_initial and correct_final:
            return 'roi_corrected_errors'
        else:
            return 'roi_unchanged_errors'
    
    def _save_performance_report(self, train_metrics: Dict, val_metrics: Dict):
        """ì„±ëŠ¥ ë¦¬í¬íŠ¸ë¥¼ JSONìœ¼ë¡œ ì €ì¥"""
        
        report = {
            'timestamp': str(Path().cwd()),
            'config': {
                'epochs': self.config.DEFAULT_EPOCHS,
                'batch_size': self.config.BATCH_SIZE,
                'learning_rate': self.config.LEARNING_RATE,
                'f1_threshold': self.config.F1_THRESHOLD
            },
            'classes': self.classes,
            'num_classes': self.num_classes,
            'train_metrics': {
                'accuracy': float(train_metrics['accuracy']),
                'weighted_precision': float(train_metrics['weighted_precision']),
                'weighted_recall': float(train_metrics['weighted_recall']),
                'weighted_f1': float(train_metrics['weighted_f1']),
                'class_wise': {
                    self.classes[i]: {
                        'precision': float(train_metrics['precision'][i]),
                        'recall': float(train_metrics['recall'][i]),
                        'f1_score': float(train_metrics['f1_scores'][i]),
                        'correct': int(train_metrics['class_corrects'][i]),
                        'total': int(train_metrics['class_totals'][i]),
                        'support': int(train_metrics['support'][i])
                    } for i in range(len(self.classes))
                },
                'confusion_matrix': train_metrics['confusion_matrix']
            },
            'validation_metrics': {
                'accuracy': float(val_metrics['accuracy']),
                'weighted_precision': float(val_metrics['weighted_precision']),
                'weighted_recall': float(val_metrics['weighted_recall']),
                'weighted_f1': float(val_metrics['weighted_f1']),
                'class_wise': {
                    self.classes[i]: {
                        'precision': float(val_metrics['precision'][i]),
                        'recall': float(val_metrics['recall'][i]),
                        'f1_score': float(val_metrics['f1_scores'][i]),
                        'correct': int(val_metrics['class_corrects'][i]),
                        'total': int(val_metrics['class_totals'][i]),
                        'support': int(val_metrics['support'][i])
                    } for i in range(len(self.classes))
                },
                'confusion_matrix': val_metrics['confusion_matrix']
            },
            'difficult_classes': self.difficult_classes
        }
        
        # JSON ì €ì¥
        report_path = self.output_dir / 'performance_report.json'
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ“‹ Performance report saved: {report_path}")
        
        # ìš”ì•½ í†µê³„ ì¶œë ¥
        print(f"\nğŸ¯ Training Summary:")
        print(f"  Train Accuracy:     {train_metrics['accuracy']:.4f} ({train_metrics['accuracy']*100:.2f}%)")
        print(f"  Val Accuracy:       {val_metrics['accuracy']:.4f} ({val_metrics['accuracy']*100:.2f}%)")
        print(f"  Train Weighted F1:  {train_metrics['weighted_f1']:.4f}")
        print(f"  Val Weighted F1:    {val_metrics['weighted_f1']:.4f}")
        print(f"  Difficult Classes:  {len(self.difficult_classes)}/{len(self.classes)}")
    
    def _save_comprehensive_performance_report(self, train_cls: Dict, val_cls: Dict, 
                                              train_roi: Dict, val_roi: Dict):
        """2ë‹¨ê³„ ì„±ëŠ¥ì„ í¬í•¨í•œ ì¢…í•© ë¦¬í¬íŠ¸ ì €ì¥"""
        
        comprehensive_report = {
            'timestamp': str(Path().cwd()),
            'config': {
                'epochs': self.config.DEFAULT_EPOCHS,
                'batch_size': self.config.BATCH_SIZE,
                'learning_rate': self.config.LEARNING_RATE,
                'f1_threshold': self.config.F1_THRESHOLD,
                'confidence_threshold': self.config.CONFIDENCE_THRESHOLD
            },
            'classes': self.classes,
            'num_classes': self.num_classes,
            'difficult_classes': self.difficult_classes,
            
            # Stage 1: Classification Only
            'stage1_classification_only': {
                'train_metrics': {
                    'accuracy': float(train_cls['accuracy']),
                    'weighted_precision': float(train_cls['weighted_precision']),
                    'weighted_recall': float(train_cls['weighted_recall']),
                    'weighted_f1': float(train_cls['weighted_f1']),
                    'class_wise': {
                        self.classes[i]: {
                            'precision': float(train_cls['precision'][i]),
                            'recall': float(train_cls['recall'][i]),
                            'f1_score': float(train_cls['f1_scores'][i]),
                            'correct': int(train_cls['class_corrects'][i]),
                            'total': int(train_cls['class_totals'][i]),
                            'support': int(train_cls['support'][i])
                        } for i in range(len(self.classes))
                    },
                    'confusion_matrix': train_cls['confusion_matrix']
                },
                'validation_metrics': {
                    'accuracy': float(val_cls['accuracy']),
                    'weighted_precision': float(val_cls['weighted_precision']),
                    'weighted_recall': float(val_cls['weighted_recall']),
                    'weighted_f1': float(val_cls['weighted_f1']),
                    'class_wise': {
                        self.classes[i]: {
                            'precision': float(val_cls['precision'][i]),
                            'recall': float(val_cls['recall'][i]),
                            'f1_score': float(val_cls['f1_scores'][i]),
                            'correct': int(val_cls['class_corrects'][i]),
                            'total': int(val_cls['class_totals'][i]),
                            'support': int(val_cls['support'][i])
                        } for i in range(len(self.classes))
                    },
                    'confusion_matrix': val_cls['confusion_matrix']
                }
            },
            
            # Stage 2: ROI Enhanced
            'stage2_roi_enhanced': {
                'roi_mappings': self.class_object_mapping,
                'train_metrics': {
                    'accuracy': float(train_roi['accuracy']),
                    'weighted_precision': float(train_roi['weighted_precision']),
                    'weighted_recall': float(train_roi['weighted_recall']),
                    'weighted_f1': float(train_roi['weighted_f1']),
                    'roi_usage_stats': train_roi['roi_usage_stats'],
                    'class_wise': {
                        self.classes[i]: {
                            'precision': float(train_roi['precision'][i]),
                            'recall': float(train_roi['recall'][i]),
                            'f1_score': float(train_roi['f1_scores'][i]),
                            'correct': int(train_roi['class_corrects'][i]),
                            'total': int(train_roi['class_totals'][i]),
                            'support': int(train_roi['support'][i])
                        } for i in range(len(self.classes))
                    },
                    'confusion_matrix': train_roi['confusion_matrix']
                },
                'validation_metrics': {
                    'accuracy': float(val_roi['accuracy']),
                    'weighted_precision': float(val_roi['weighted_precision']),
                    'weighted_recall': float(val_roi['weighted_recall']),
                    'weighted_f1': float(val_roi['weighted_f1']),
                    'roi_usage_stats': val_roi['roi_usage_stats'],
                    'class_wise': {
                        self.classes[i]: {
                            'precision': float(val_roi['precision'][i]),
                            'recall': float(val_roi['recall'][i]),
                            'f1_score': float(val_roi['f1_scores'][i]),
                            'correct': int(val_roi['class_corrects'][i]),
                            'total': int(val_roi['class_totals'][i]),
                            'support': int(val_roi['support'][i])
                        } for i in range(len(self.classes))
                    },
                    'confusion_matrix': val_roi['confusion_matrix']
                }
            },
            
            # Performance Comparison
            'performance_comparison': {
                'accuracy_improvement': float(val_roi['accuracy'] - val_cls['accuracy']),
                'weighted_f1_improvement': float(val_roi['weighted_f1'] - val_cls['weighted_f1']),
                'class_wise_improvements': {
                    self.classes[i]: {
                        'f1_improvement': float(val_roi['f1_scores'][i] - val_cls['f1_scores'][i])
                    } for i in range(len(self.classes))
                }
            }
        }
        
        # JSON ì €ì¥
        report_path = self.output_dir / 'comprehensive_performance_report.json'
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(comprehensive_report, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ“‹ Comprehensive performance report saved: {report_path}")
        
        # ìš”ì•½ í†µê³„ ì¶œë ¥
        print(f"\nğŸ¯ Final Training Summary:")
        print(f"  Stage 1 - Classification Only:")
        print(f"    Train Accuracy:     {train_cls['accuracy']:.4f} ({train_cls['accuracy']*100:.2f}%)")
        print(f"    Val Accuracy:       {val_cls['accuracy']:.4f} ({val_cls['accuracy']*100:.2f}%)")
        print(f"    Val Weighted F1:    {val_cls['weighted_f1']:.4f}")
        
        print(f"  Stage 2 - ROI Enhanced:")
        print(f"    Train Accuracy:     {train_roi['accuracy']:.4f} ({train_roi['accuracy']*100:.2f}%)")
        print(f"    Val Accuracy:       {val_roi['accuracy']:.4f} ({val_roi['accuracy']*100:.2f}%)")
        print(f"    Val Weighted F1:    {val_roi['weighted_f1']:.4f}")
        
        acc_improvement = val_roi['accuracy'] - val_cls['accuracy']
        f1_improvement = val_roi['weighted_f1'] - val_cls['weighted_f1']
        
        print(f"  Performance Improvement:")
        print(f"    Accuracy:           {acc_improvement:+.4f} ({acc_improvement*100:+.2f}%)")
        print(f"    Weighted F1:        {f1_improvement:+.4f}")
        print(f"    Difficult Classes:  {len(self.difficult_classes)}/{len(self.classes)}")
    
    def _create_roi_mappings(self):
        """ROI ë§¤í•‘ ìƒì„± - ì „ì²´ validation ë°ì´í„°ë¡œ í´ë˜ìŠ¤ë³„ YOLO ê°ì²´ ë§¤í•‘"""
        
        if self.yolo_model is None:
            print("âš ï¸ YOLO model not loaded, skipping ROI mappings")
            return
        
        print("ğŸ”— Creating ROI mappings with full validation dataset...")
        
        # ê° í´ë˜ìŠ¤ë³„ ê°ì²´ ì¶œí˜„ íšŸìˆ˜ ì¹´ìš´í„°
        class_object_counts = {cls: {} for cls in self.classes}
        class_total_counts = {cls: 0 for cls in self.classes}
        
        total_processed = 0
        
        # ì›ë³¸ ì´ë¯¸ì§€ì—ì„œ ì§ì ‘ ROI ë§¤í•‘ ìƒì„±
        # validation datasetì˜ ì´ë¯¸ì§€ íŒŒì¼ë“¤ì„ ì§ì ‘ ìˆœíšŒ
        val_dataset_root = self.dataset_root
        
        for class_name in self.classes:
            class_dir = val_dataset_root / class_name
            if not class_dir.exists():
                continue
                
            image_files = []
            for ext in ['.jpg', '.jpeg', '.png', '.bmp']:
                image_files.extend(class_dir.glob(f"*{ext}"))
                image_files.extend(class_dir.glob(f"*{ext.upper()}"))
            
            print(f"  Processing class '{class_name}': {len(image_files)} images")
            
            for img_path in image_files:
                class_total_counts[class_name] += 1
                total_processed += 1
                
                try:
                    # í•™ìŠµëœ í´ë˜ìŠ¤ë³„ ROI íŒ¨í„´ì„ ì‚¬ìš©í•˜ì—¬ ROI ì¶”ì¶œ
                    roi_image = self._extract_roi_with_learned_pattern(str(img_path), class_name)
                    results = self.yolo_model(roi_image, verbose=False)
                    
                    if len(results) > 0 and len(results[0].boxes) > 0:
                        # ëª¨ë“  ê²€ì¶œëœ ê°ì²´ ì¤‘ ê°€ì¥ ì‹ ë¢°ë„ ë†’ì€ ê°ì²´ ì„ íƒ
                        confidences = results[0].boxes.conf.cpu().numpy()
                        classes = results[0].boxes.cls.cpu().numpy()
                        
                        # ì‹ ë¢°ë„ ì„ê³„ê°’ ì´ìƒì¸ ê°ì²´ë“¤ë§Œ ê³ ë ¤
                        valid_indices = confidences > self.config.OBJECT_CONFIDENCE_THRESHOLD
                        
                        if np.any(valid_indices):
                            valid_classes = classes[valid_indices]
                            
                            # ê° ê°ì²´ í´ë˜ìŠ¤ë³„ ê°œìˆ˜ ì„¸ê¸°
                            unique_classes, counts = np.unique(valid_classes, return_counts=True)
                            
                            # ê°€ì¥ ë§ì´ ê²€ì¶œëœ ê°ì²´ í´ë˜ìŠ¤ ì„ íƒ
                            most_frequent_idx = np.argmax(counts)
                            detected_class = int(unique_classes[most_frequent_idx])
                            object_count = counts[most_frequent_idx]
                            
                            if detected_class < len(self.yolo_objects):
                                object_name = self.yolo_objects[detected_class]
                                print(f"ğŸ” [{class_name}] Most frequent object: '{object_name}' (count: {object_count})")
                                
                                # í´ë˜ìŠ¤ë³„ ê°ì²´ ì¹´ìš´íŠ¸ ì¦ê°€
                                if object_name not in class_object_counts[class_name]:
                                    class_object_counts[class_name][object_name] = 0
                                class_object_counts[class_name][object_name] += 1
                
                except Exception as e:
                    # ê°œë³„ ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹¤íŒ¨ëŠ” ë¬´ì‹œí•˜ê³  ê³„ì† ì§„í–‰
                    continue
        
        # í†µê³„ ë° ë§¤í•‘ ìƒì„±
        print(f"\nğŸ“Š ROI mapping statistics:")
        print(f"  Total processed: {total_processed} images")
        
        self.class_object_mapping = {}
        mapping_details = {}
        
        for class_name in self.classes:
            class_total = class_total_counts[class_name]
            object_counts = class_object_counts[class_name]
            
            print(f"\nğŸ” Class '{class_name}': {class_total} images")
            
            if not object_counts or class_total == 0:
                print(f"    âŒ No objects detected")
                continue
            
            # ê°ì²´ë³„ ì¶œí˜„ ë¹„ìœ¨ ê³„ì‚° (ì „ì²´ í´ë˜ìŠ¤ ì´ë¯¸ì§€ ëŒ€ë¹„)
            object_ratios = {}
            for object_name, count in object_counts.items():
                ratio = count / class_total
                object_ratios[object_name] = ratio
                print(f"    ğŸ“¦ {object_name}: {count}/{class_total} = {ratio:.3f} ({ratio*100:.1f}%)")
            
            # ê°€ì¥ ë†’ì€ ë¹„ìœ¨ì˜ ê°ì²´ ì°¾ê¸°
            if object_ratios:
                best_object = max(object_ratios.items(), key=lambda x: x[1])
                best_object_name, best_ratio = best_object
                
                # ì„ê³„ê°’ ì´ìƒì´ë©´ ë§¤í•‘ ìƒì„±
                if best_ratio >= self.config.MAPPING_THRESHOLD:
                    self.class_object_mapping[class_name] = best_object_name
                    print(f"    âœ… Mapping created: {class_name} â†’ {best_object_name} ({best_ratio:.3f})")
                else:
                    print(f"    âŒ Best ratio {best_ratio:.3f} < threshold {self.config.MAPPING_THRESHOLD}")
                
                # ìƒì„¸ ì •ë³´ ì €ì¥
                mapping_details[class_name] = {
                    'total_images': class_total,
                    'object_counts': object_counts,
                    'object_ratios': object_ratios,
                    'best_object': best_object_name,
                    'best_ratio': best_ratio,
                    'mapping_created': best_ratio >= self.config.MAPPING_THRESHOLD
                }
        
        print(f"\nâœ… ROI mappings created: {len(self.class_object_mapping)}/{len(self.classes)}")
        for cls, obj in self.class_object_mapping.items():
            ratio = mapping_details[cls]['best_ratio']
            print(f"  ğŸ¯ {cls} â†’ {obj} ({ratio:.3f})")
        
        # ë§¤í•‘ ê²°ê³¼ ì €ì¥ (ìƒì„¸ ì •ë³´ í¬í•¨)
        mapping_path = self.output_dir / self.config.MAPPING_RESULTS_FILE
        with open(mapping_path, 'w', encoding='utf-8') as f:
            json.dump({
                'difficult_classes': self.difficult_classes,
                'class_object_mapping': self.class_object_mapping,
                'yolo_objects': self.yolo_objects,
                'mapping_details': mapping_details,
                'total_processed_images': total_processed,
                'class_total_counts': class_total_counts,
                'mapping_threshold': self.config.MAPPING_THRESHOLD
            }, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ’¾ ROI mappings saved: {mapping_path}")
    
    def _load_model_for_inference(self):
        """ì¶”ë¡ ìš© ëª¨ë¸ ë¡œë“œ"""
        
        # í´ë˜ìŠ¤ ì •ë³´ ë¡œë“œ
        info_path = self.output_dir / 'class_info.json'
        if not info_path.exists():
            raise FileNotFoundError(f"Class info not found: {info_path}")
        
        with open(info_path, 'r', encoding='utf-8') as f:
            class_info = json.load(f)
        
        self.classes = class_info['classes']
        self.num_classes = class_info['num_classes']
        self.class_to_idx = class_info['class_to_idx']
        
        # ëª¨ë¸ ìƒì„±
        self.model = timm.create_model(
            class_info['config']['CONVNEXT_MODEL_NAME'],
            pretrained=False,
            num_classes=self.num_classes
        )
        
        # ê°€ì¤‘ì¹˜ ë¡œë“œ
        model_path = self.output_dir / self.config.CLASSIFICATION_MODEL_NAME
        if not model_path.exists():
            raise FileNotFoundError(f"Model weights not found: {model_path}")
        
        checkpoint = torch.load(model_path, map_location=self.device)
        
        # model. prefix ì œê±°
        new_state_dict = {}
        for key, value in checkpoint.items():
            if key.startswith('model.'):
                new_key = key[6:]  # "model." ì œê±°
                new_state_dict[new_key] = value
            else:
                new_state_dict[key] = value
        
        self.model.load_state_dict(new_state_dict, strict=True)
        self.model.to(self.device)
        self.model.eval()
        
        # ROI ë§¤í•‘ ì •ë³´ ë¡œë“œ
        self._load_roi_mappings()
        
        # YOLO ëª¨ë¸ ë¡œë“œ
        if not self.yolo_model:
            self._load_yolo_model()
        
        print(f"âœ… Model loaded for inference:")
        print(f"  - Classes: {self.num_classes}")
        print(f"  - Architecture: {class_info['config']['CONVNEXT_MODEL_NAME']}")
        print(f"  - Difficult classes: {len(self.difficult_classes)}")
        print(f"  - ROI mappings: {len(self.class_object_mapping)}")
    
    def _load_roi_mappings(self):
        """ROI ë§¤í•‘ ì •ë³´ ë¡œë“œ"""
        
        mapping_path = self.output_dir / self.config.MAPPING_RESULTS_FILE
        if not mapping_path.exists():
            print("âš ï¸ No ROI mappings found, using classification only")
            self.difficult_classes = []
            self.class_object_mapping = {}
            return
        
        with open(mapping_path, 'r', encoding='utf-8') as f:
            mapping_data = json.load(f)
        
        self.difficult_classes = mapping_data.get('difficult_classes', [])
        self.class_object_mapping = mapping_data.get('class_object_mapping', {})
        self.yolo_objects = mapping_data.get('yolo_objects', [])
    
    def predict(self, image_path: str) -> Dict[str, Any]:
        """ë‹¨ì¼ ì´ë¯¸ì§€ ì˜ˆì¸¡ - ROI ê²€ì¦ í¬í•¨"""
        
        if self.model is None:
            self._load_model_for_inference()
        
        # ImageFolder ë°©ì‹ìœ¼ë¡œ ë‹¨ì¼ ì´ë¯¸ì§€ ì²˜ë¦¬
        image_path = Path(image_path)
        if image_path.is_file():
            # íŒŒì¼ëª… ì…ë ¥ ê²½ìš° - ì§ì ‘ ë¡œë“œ (ì˜ˆì™¸)
            transform = transforms.Compose([
                transforms.Resize((self.config.CLASSIFICATION_SIZE, self.config.CLASSIFICATION_SIZE)),
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
            ])
            image = Image.open(image_path).convert('RGB')
            input_tensor = transform(image).unsqueeze(0).to(self.device)
        else:
            # í´ë” ê²½ìš° - ImageFolder ì‚¬ìš©
            transform = transforms.Compose([
                transforms.Resize((self.config.CLASSIFICATION_SIZE, self.config.CLASSIFICATION_SIZE)),
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
            ])
            temp_dataset = datasets.ImageFolder(root=image_path, transform=transform)
            if len(temp_dataset) == 0:
                raise ValueError(f"No images found in {image_path}")
            input_tensor, _ = temp_dataset[0]
            input_tensor = input_tensor.unsqueeze(0).to(self.device)
        
        # 1. Classification ì˜ˆì¸¡
        with torch.no_grad():
            outputs = self.model(input_tensor)
            probabilities = torch.softmax(outputs, dim=1).cpu().numpy()[0]
            predicted_idx = np.argmax(probabilities)
            initial_confidence = float(probabilities[predicted_idx])
        
        predicted_class = self.classes[predicted_idx]
        final_confidence = initial_confidence
        used_roi_verification = False
        roi_verification_success = None
        
        # 2. ROI ê²€ì¦ í•„ìš”í•œì§€ í™•ì¸
        needs_roi_verification = (
            predicted_class in self.difficult_classes and
            initial_confidence < self.config.CONFIDENCE_THRESHOLD and
            len(self.class_object_mapping) > 0 and  # ROI ë§¤í•‘ì´ ì¡´ì¬í•˜ë©´
            self.yolo_model is not None
        )
        
        if needs_roi_verification:
            # 3. ROI ê¸°ë°˜ í´ë˜ìŠ¤ ì¬ê²°ì •
            used_roi_verification = True
            roi_suggested_class = self._get_roi_suggested_class(image_path)
            
            if roi_suggested_class:
                # 4. ROIë¡œ í´ë˜ìŠ¤ ì™„ì „ ë³€ê²½
                predicted_class = roi_suggested_class
                predicted_idx = self.classes.index(roi_suggested_class)
                final_confidence = self.config.ROI_CONFIDENCE_BOOST + 0.6  # ë†’ì€ ì‹ ë¢°ë„ ë¶€ì—¬
                roi_verification_success = True
            else:
                # ROI ê²€ì¶œ ì‹¤íŒ¨ ì‹œ ì›ë˜ ê²°ê³¼ ìœ ì§€í•˜ë˜ ì‹ ë¢°ë„ í•˜ë½
                final_confidence = max(0.0, initial_confidence - self.config.ROI_CONFIDENCE_PENALTY)
                roi_verification_success = False
        
        return {
            'image_path': image_path,
            'predicted_class': predicted_class,
            'initial_confidence': initial_confidence,
            'final_confidence': final_confidence,
            'confidence': final_confidence,  # í˜¸í™˜ì„±
            'class_idx': predicted_idx,
            'used_roi_verification': used_roi_verification,
            'roi_verification_success': roi_verification_success,
            'all_probabilities': {self.classes[i]: float(probabilities[i]) for i in range(len(self.classes))}
        }
    
    def _get_roi_suggested_class(self, image_path: str) -> Optional[str]:
        """ROI ê¸°ë°˜ í´ë˜ìŠ¤ ì œì•ˆ"""
        
        try:
            # ì›ë³¸ ì´ë¯¸ì§€ë¥¼ ROI cropí•œ í›„ YOLO ê²€ì¶œ
            original_image = Image.open(image_path).convert('RGB')
            image_np = np.array(original_image)
            roi_image = self._extract_roi_region(image_np)
            results = self.yolo_model(roi_image, verbose=False)
            
            if len(results) == 0 or len(results[0].boxes) == 0:
                return None
            
            # ê°€ì¥ ì‹ ë¢°ë„ ë†’ì€ ê°ì²´ ì°¾ê¸°
            confidences = results[0].boxes.conf.cpu().numpy()
            classes = results[0].boxes.cls.cpu().numpy()
            
            max_conf_idx = np.argmax(confidences)
            if confidences[max_conf_idx] < self.config.OBJECT_CONFIDENCE_THRESHOLD:
                return None
            
            # ê²€ì¶œëœ ê°ì²´ ì´ë¦„
            detected_class = int(classes[max_conf_idx])
            detected_object = self.yolo_objects[detected_class]
            
            # ì—­ë§¤í•‘: ê²€ì¶œëœ ê°ì²´ê°€ ì–´ë–¤ í´ë˜ìŠ¤ì™€ ì—°ê²°ë˜ëŠ”ì§€ ì°¾ê¸°
            for class_name, mapped_object in self.class_object_mapping.items():
                if mapped_object == detected_object:
                    return class_name
            
            # ë§¤í•‘ì— ì—†ëŠ” ê°ì²´ë©´ None ë°˜í™˜
            return None
            
        except Exception as e:
            print(f"âš ï¸ ROI class suggestion failed: {e}")
            return None
    
    def predict_batch(self, image_paths: List[str]) -> List[Dict[str, Any]]:
        """ë°°ì¹˜ ì˜ˆì¸¡"""
        
        results = []
        for image_path in image_paths:
            try:
                result = self.predict(image_path)
                results.append(result)
            except Exception as e:
                results.append({
                    'image_path': image_path,
                    'error': str(e),
                    'predicted_class': None,
                    'confidence': 0.0
                })
        
        return results
    
    def evaluate_dataset(self, dataset_path: str) -> Dict[str, Any]:
        """ImageFolder êµ¬ì¡°ì˜ ë°ì´í„°ì…‹ì„ í‰ê°€í•˜ì—¬ ì„±ëŠ¥ ë¶„ì„ ê²°ê³¼ ë°˜í™˜
        
        Args:
            dataset_path: ImageFolder êµ¬ì¡°ì˜ ë°ì´í„°ì…‹ ê²½ë¡œ
            
        Returns:
            Dict: ì •í™•ë„, F1 ì ìˆ˜, í´ë˜ìŠ¤ë³„ ì„±ëŠ¥, ì˜ˆì¸¡ ê²°ê³¼ ë“±
        """
        from torchvision import datasets, transforms
        from sklearn.metrics import accuracy_score, f1_score, precision_recall_fscore_support
        
        # ëª¨ë¸ ë¡œë“œ
        self._load_model_for_inference()
        
        # ë°ì´í„°ì…‹ ë¡œë“œ (ImageFolder ì‚¬ìš©)
        transform = transforms.Compose([
            transforms.Resize((384, 384)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
        
        dataset = datasets.ImageFolder(dataset_path, transform=transform)
        dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=False)
        
        # ì˜ˆì¸¡ ìˆ˜í–‰
        all_predictions = []
        all_true_labels = []
        all_confidences = []
        all_image_paths = []
        
        self.model.eval()
        with torch.no_grad():
            for batch_idx, (images, labels) in enumerate(dataloader):
                images = images.to(self.device)
                
                outputs = self.model(images)
                probabilities = torch.nn.functional.softmax(outputs, dim=1)
                predicted_classes = torch.argmax(probabilities, dim=1)
                confidences = torch.max(probabilities, dim=1)[0]
                
                # ë°°ì¹˜ ê²°ê³¼ ì €ì¥
                for i in range(len(images)):
                    img_idx = batch_idx * dataloader.batch_size + i
                    if img_idx < len(dataset.samples):
                        image_path, true_label = dataset.samples[img_idx]
                        
                        all_predictions.append(predicted_classes[i].cpu().item())
                        all_true_labels.append(labels[i].cpu().item())
                        all_confidences.append(confidences[i].cpu().item())
                        all_image_paths.append(image_path)
        
        # ì„±ëŠ¥ ê³„ì‚°
        accuracy = accuracy_score(all_true_labels, all_predictions)
        weighted_f1 = f1_score(all_true_labels, all_predictions, average='weighted')
        
        # í´ë˜ìŠ¤ë³„ ì„±ëŠ¥
        precision, recall, f1, support = precision_recall_fscore_support(
            all_true_labels, all_predictions, average=None, zero_division=0
        )
        
        class_names = list(dataset.class_to_idx.keys())
        class_metrics = {}
        for i, class_name in enumerate(class_names):
            class_metrics[class_name] = {
                'precision': precision[i] if i < len(precision) else 0.0,
                'recall': recall[i] if i < len(recall) else 0.0,
                'f1': f1[i] if i < len(f1) else 0.0,
                'support': support[i] if i < len(support) else 0
            }
        
        # ì˜ˆì¸¡ ê²°ê³¼ ìƒì„¸ ì •ë³´
        sample_predictions = []
        for i in range(len(all_predictions)):
            pred_class_name = class_names[all_predictions[i]]
            true_class_name = class_names[all_true_labels[i]]
            
            sample_predictions.append({
                'image_path': all_image_paths[i],
                'predicted_class': pred_class_name,
                'true_class': true_class_name,
                'confidence': all_confidences[i],
                'correct': all_predictions[i] == all_true_labels[i]
            })
        
        return {
            'accuracy': accuracy,
            'weighted_f1': weighted_f1,
            'class_metrics': class_metrics,
            'sample_predictions': sample_predictions,
            'total_samples': len(all_predictions)
        }


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    
    parser = argparse.ArgumentParser(
        description="Enhanced Wafer Defect Detection - Training & Inference",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  # í›ˆë ¨ (ê¸°ë³¸ ë™ì‘)
  python train.py
  python train.py dataset_path/
  python train.py --epochs 50 --mode production
  
  # ì˜ˆì¸¡
  python train.py --predict image.jpg        # ë‹¨ì¼ ì´ë¯¸ì§€
  python train.py --predict folder/          # ë‹¨ìˆœ í´ë” ë°°ì¹˜ ì²˜ë¦¬
  python train.py --predict test_dataset/    # ImageFolder êµ¬ì¡° ì„±ëŠ¥ í‰ê°€
        """
    )
    
    parser.add_argument("dataset_path", nargs='?', help="ë°ì´í„°ì…‹ ë£¨íŠ¸ ê²½ë¡œ (ê¸°ë³¸ê°’: config.pyì˜ DATASET_ROOT)")
    
    # ì‘ì—… ëª¨ë“œ (ê¸°ë³¸ê°’: í›ˆë ¨)
    parser.add_argument("--predict", help="ì´ë¯¸ì§€/í´ë” ì˜ˆì¸¡ (ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ ìë™ìœ¼ë¡œ í›ˆë ¨ ì‹¤í–‰)")
    
    # í›ˆë ¨ ì˜µì…˜
    parser.add_argument("--epochs", type=int, help="í›ˆë ¨ ì—í¬í¬ ìˆ˜")
    parser.add_argument("--mode", choices=['quick-test', 'production'], default='production', help="í›ˆë ¨ ëª¨ë“œ")
    
    # ì˜ˆì¸¡ ì˜µì…˜
    parser.add_argument("--batch", action="store_true", help="[ë” ì´ìƒ í•„ìš” ì—†ìŒ] í´ë”ëŠ” ìë™ìœ¼ë¡œ ë°°ì¹˜ ì²˜ë¦¬ë¨")
    parser.add_argument("--output-dir", help="ëª¨ë¸ ì¶œë ¥ ë””ë ‰í† ë¦¬")
    
    args = parser.parse_args()
    
    print("ğŸ¯ Enhanced Wafer Defect Detection")
    print("=" * 40)
    
    # ë°ì´í„°ì…‹ ê²½ë¡œ ì„¤ì • (configì—ì„œ ê¸°ë³¸ê°’ ì‚¬ìš© ê°€ëŠ¥)
    if args.dataset_path:
        dataset_path = Path(args.dataset_path)
    else:
        # configì—ì„œ ê¸°ë³¸ ë°ì´í„°ì…‹ ê²½ë¡œ ê°€ì ¸ì˜¤ê¸°
        from config import ConfigManager
        temp_config = ConfigManager()
        if not temp_config.get_config().DATASET_ROOT:
            print("âŒ ë°ì´í„°ì…‹ ê²½ë¡œë¥¼ ì§€ì •í•˜ê±°ë‚˜ config.pyì˜ DATASET_ROOTë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
            print("   ì‚¬ìš©ë²•: python train.py your_dataset_path/ --train")
            print("   ë˜ëŠ” config.pyì—ì„œ DATASET_ROOT = 'your_dataset_path' ì„¤ì •")
            return 1
        dataset_path = Path(temp_config.get_config().DATASET_ROOT)
        print(f"ğŸ“‚ Using dataset from config: {dataset_path}")
    
    if not dataset_path.exists():
        print(f"âŒ Dataset not found: {dataset_path}")
        return 1
    
    # ì„¤ì • ì ìš©
    if args.mode == 'quick-test':
        config_manager = get_quick_test_config(str(dataset_path))
    else:
        config_manager = get_production_config(str(dataset_path))
    
    if args.output_dir:
        config_manager.get_config().OUTPUT_DIR = args.output_dir
    
    # í›ˆë ¨ê¸° ì´ˆê¸°í™”
    trainer = WaferTrainer(str(dataset_path), config_manager)
    
    try:
        if args.predict:
            # ì˜ˆì¸¡ ëª¨ë“œ
            predict_path = Path(args.predict)
            
            if not predict_path.exists():
                print(f"âŒ Path not found: {predict_path}")
                return 1
            
            if predict_path.is_dir():
                # í´ë” ì˜ˆì¸¡ (ImageFolder êµ¬ì¡°ë¡œ ì²˜ë¦¬)
                print(f"ğŸ“ Folder detected: {predict_path}")
                
                # ImageFolder êµ¬ì¡°ì¸ì§€ í™•ì¸ (í•˜ìœ„ì— í´ë˜ìŠ¤ í´ë”ë“¤ì´ ìˆëŠ”ì§€)
                subfolders = [d for d in predict_path.iterdir() if d.is_dir()]
                
                if subfolders:
                    # ImageFolder êµ¬ì¡°: ë¼ë²¨ì´ ìˆëŠ” ë°ì´í„°ì…‹ìœ¼ë¡œ ì²˜ë¦¬
                    print(f"ğŸ·ï¸ ImageFolder structure detected with {len(subfolders)} classes")
                    print(f"ğŸ“Š Classes found: {[d.name for d in subfolders]}")
                    
                    # ë°ì´í„°ì…‹ í‰ê°€ ëª¨ë“œë¡œ ì²˜ë¦¬
                    results = trainer.evaluate_dataset(str(predict_path))
                    
                    # ì„±ëŠ¥ ë¶„ì„ ê²°ê³¼ ì¶œë ¥
                    print(f"\nğŸ“Š Dataset Evaluation Results:")
                    print("=" * 60)
                    print(f"ğŸ¯ Overall Accuracy: {results['accuracy']:.3f}")
                    print(f"ğŸ“Š Weighted F1 Score: {results['weighted_f1']:.3f}")
                    
                    print(f"\nğŸ“‹ Class-wise Performance:")
                    print("-" * 40)
                    for class_name, metrics in results['class_metrics'].items():
                        print(f"  {class_name}:")
                        print(f"    Precision: {metrics['precision']:.3f}")
                        print(f"    Recall:    {metrics['recall']:.3f}")
                        print(f"    F1 Score:  {metrics['f1']:.3f}")
                    
                    print(f"\nğŸ” Sample Predictions:")
                    print("-" * 40)
                    for i, pred in enumerate(results['sample_predictions'][:10]):  # ì²˜ìŒ 10ê°œë§Œ í‘œì‹œ
                        rel_path = Path(pred['image_path']).relative_to(predict_path)
                        status = "âœ…" if pred['predicted_class'] == pred['true_class'] else "âŒ"
                        print(f"  {status} {rel_path}: {pred['predicted_class']} (conf: {pred['confidence']:.3f}) [True: {pred['true_class']}]")
                    
                    if len(results['sample_predictions']) > 10:
                        print(f"  ... and {len(results['sample_predictions']) - 10} more predictions")
                
                else:
                    # ë‹¨ìˆœ í´ë”: ë¼ë²¨ ì—†ëŠ” ë°°ì¹˜ ì²˜ë¦¬
                    print("ğŸ“ Simple folder structure (no class labels)")
                    
                    # ëª¨ë“  ì´ë¯¸ì§€ íŒŒì¼ ì°¾ê¸°
                    image_extensions = ['.jpg', '.jpeg', '.png', '.bmp']
                    image_files = []
                    
                    for ext in image_extensions:
                        image_files.extend(predict_path.glob(f"*{ext}"))
                        image_files.extend(predict_path.glob(f"*{ext.upper()}"))
                    
                    if not image_files:
                        print("âŒ No image files found in folder")
                        return 1
                    
                    print(f"ğŸ” Found {len(image_files)} images for batch prediction")
                    results = trainer.predict_batch([str(f) for f in image_files])
                    
                    # ê²°ê³¼ ì¶œë ¥
                    print(f"\nğŸ“Š Batch Prediction Results:")
                    print("=" * 50)
                    for result in results:
                        if 'error' not in result:
                            print(f"  {Path(result['image_path']).name}: {result['predicted_class']} ({result['confidence']:.3f})")
                        else:
                            print(f"  {Path(result['image_path']).name}: ERROR - {result['error']}")
            
            else:
                # ë‹¨ì¼ íŒŒì¼ ì˜ˆì¸¡
                print(f"ğŸ” Single image prediction: {predict_path}")
                result = trainer.predict(str(predict_path))
                
                print(f"\nğŸ¯ Prediction Result:")
                print(f"  - Class: {result['predicted_class']}")
                print(f"  - Confidence: {result['confidence']:.3f}")
                print(f"  - All probabilities:")
                for cls, prob in result['all_probabilities'].items():
                    print(f"    {cls}: {prob:.3f}")
        else:
            # ì¶”ë¡  íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (ê¸°ë³¸ ë™ì‘)
            trainer.run_inference_pipeline()
    
    except Exception as e:
        print(f"âŒ Error: {e}")
        import traceback
        traceback.print_exc()
        return 1
    
    return 0


if __name__ == "__main__":
    exit(main()) 