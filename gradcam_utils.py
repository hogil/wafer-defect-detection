import torch
import torch.nn.functional as F
import cv2
import numpy as np
from typing import Tuple, Dict, List
import timm
from PIL import Image


class GradCAMAnalyzer:
    """Classification ëª¨ë¸ì´ ì‹¤ì œë¡œ ì–´ë””ë¥¼ ë³´ê³  ì˜ˆì¸¡í–ˆëŠ”ì§€ ë¶„ì„í•˜ëŠ” Grad-CAM êµ¬í˜„"""
    
    def __init__(self, model, target_layer_name: str = None):
        """
        Args:
            model: í›ˆë ¨ëœ Classification ëª¨ë¸
            target_layer_name: Grad-CAMì„ ì ìš©í•  ë ˆì´ì–´ ì´ë¦„ (Noneì´ë©´ ìë™ ì„ íƒ)
        """
        self.model = model
        self.model.eval()
        
        # ConvNeXtV2ì˜ ë§ˆì§€ë§‰ conv layer ì°¾ê¸°
        if target_layer_name is None:
            target_layer_name = self._find_target_layer()
        
        self.target_layer = self._get_layer_by_name(target_layer_name)
        self.gradients = None
        self.activations = None
        
        # Hook ë“±ë¡
        self._register_hooks()
    
    def _find_target_layer(self) -> str:
        """ConvNeXtV2 ëª¨ë¸ì—ì„œ ì ì ˆí•œ target layer ì°¾ê¸°"""
        # ConvNeXtV2ì˜ ë§ˆì§€ë§‰ feature extraction layer
        for name, module in self.model.named_modules():
            if 'stages' in name and 'blocks' in name and len(list(module.children())) == 0:
                continue
        
        # ì¼ë°˜ì ìœ¼ë¡œ ë§ˆì§€ë§‰ convolutional stage
        return "stages.3"  # ConvNeXtV2ì˜ ë§ˆì§€ë§‰ stage
    
    def _get_layer_by_name(self, layer_name: str):
        """ë ˆì´ì–´ ì´ë¦„ìœ¼ë¡œ ì‹¤ì œ ë ˆì´ì–´ ê°ì²´ ê°€ì ¸ì˜¤ê¸°"""
        names = layer_name.split('.')
        layer = self.model
        for name in names:
            layer = getattr(layer, name)
        return layer
    
    def _register_hooks(self):
        """Forward/Backward hook ë“±ë¡"""
        def forward_hook(module, input, output):
            self.activations = output
        
        def backward_hook(module, grad_input, grad_output):
            self.gradients = grad_output[0]
        
        self.target_layer.register_forward_hook(forward_hook)
        self.target_layer.register_backward_hook(backward_hook)
    
    def generate_gradcam(self, image_tensor: torch.Tensor, target_class: int = None) -> np.ndarray:
        """
        íŠ¹ì • í´ë˜ìŠ¤ì— ëŒ€í•œ Grad-CAM ìƒì„±
        
        Args:
            image_tensor: ì…ë ¥ ì´ë¯¸ì§€ í…ì„œ (1, C, H, W)
            target_class: ë¶„ì„í•  í´ë˜ìŠ¤ (Noneì´ë©´ ì˜ˆì¸¡ëœ í´ë˜ìŠ¤)
            
        Returns:
            np.ndarray: Grad-CAM heatmap (H, W)
        """
        # ëª¨ë¸ì„ train ëª¨ë“œë¡œ ì„¤ì • (gradient ê³„ì‚°ì„ ìœ„í•´)
        original_mode = self.model.training
        self.model.train()
        
        # Forward pass
        image_tensor.requires_grad_(True)
        output = self.model(image_tensor)
        
        if target_class is None:
            target_class = output.argmax(dim=1).item()
        
        # Backward pass
        self.model.zero_grad()
        class_score = output[0, target_class]
        
        # gradient ê³„ì‚° í™•ì¸
        if not class_score.requires_grad:
            print(f"âš ï¸ Warning: class_score does not require grad")
            # ëª¨ë¸ì„ ì›ë˜ ëª¨ë“œë¡œ ë³µì›
            self.model.train(original_mode)
            return np.zeros((image_tensor.shape[2], image_tensor.shape[3]))
        
        class_score.backward()
        
        # Grad-CAM ê³„ì‚°
        gradients = self.gradients[0]  # (C, H, W)
        activations = self.activations[0]  # (C, H, W)
        
        # Global Average Pooling of gradients
        weights = torch.mean(gradients, dim=(1, 2))  # (C,)
        
        # Weighted combination
        gradcam = torch.zeros(activations.shape[1:])  # (H, W)
        for i, weight in enumerate(weights):
            gradcam += weight * activations[i]
        
        # ReLU
        gradcam = F.relu(gradcam)
        
        # Normalize
        if gradcam.max() > 0:
            gradcam = gradcam / gradcam.max()
        
        # ëª¨ë¸ì„ ì›ë˜ ëª¨ë“œë¡œ ë³µì›
        self.model.train(original_mode)
        
        return gradcam.detach().cpu().numpy()
    
    def get_roi_from_gradcam(self, gradcam: np.ndarray, threshold: float = 0.5, 
                           min_area_ratio: float = 0.1) -> Tuple[int, int, int, int]:
        """
        Grad-CAMì—ì„œ ROI ì˜ì—­ ì¶”ì¶œ
        
        Args:
            gradcam: Grad-CAM heatmap
            threshold: ì¤‘ìš”ë„ ì„ê³„ê°’ (0.0~1.0)
            min_area_ratio: ìµœì†Œ ROI ë©´ì  ë¹„ìœ¨
            
        Returns:
            tuple: (x1, y1, x2, y2) ROI ì¢Œí‘œ
        """
        # ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
        h, w = gradcam.shape
        
        # ì„ê³„ê°’ ì´ìƒì¸ ì˜ì—­ ì°¾ê¸°
        mask = (gradcam >= threshold).astype(np.uint8)
        
        # ì—°ê²°ëœ ì»´í¬ë„ŒíŠ¸ ì°¾ê¸°
        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        if not contours:
            # ì„ê³„ê°’ì„ ë‚®ì¶°ì„œ ë‹¤ì‹œ ì‹œë„
            threshold = 0.3
            mask = (gradcam >= threshold).astype(np.uint8)
            contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        if not contours:
            # ì—¬ì „íˆ ì—†ìœ¼ë©´ ì¤‘ì•™ ì˜ì—­ ë°˜í™˜
            center_x, center_y = w // 2, h // 2
            size = min(w, h) // 2
            return max(0, center_x - size), max(0, center_y - size), \
                   min(w, center_x + size), min(h, center_y + size)
        
        # ê°€ì¥ í° contour ì„ íƒ
        largest_contour = max(contours, key=cv2.contourArea)
        x, y, w_roi, h_roi = cv2.boundingRect(largest_contour)
        
        # ìµœì†Œ ë©´ì  ì²´í¬
        min_area = min_area_ratio * w * h
        if w_roi * h_roi < min_area:
            # ë„ˆë¬´ ì‘ìœ¼ë©´ í™•ì¥
            center_x, center_y = x + w_roi // 2, y + h_roi // 2
            expand_size = int(np.sqrt(min_area) // 2)
            x = max(0, center_x - expand_size)
            y = max(0, center_y - expand_size)
            w_roi = min(w - x, expand_size * 2)
            h_roi = min(h - y, expand_size * 2)
        
        return x, y, x + w_roi, y + h_roi
    
    def analyze_class_attention_patterns(self, dataloader, class_names: List[str], 
                                       num_samples_per_class: int = 10) -> Dict[str, List[Tuple]]:
        """
        ê° í´ë˜ìŠ¤ë³„ë¡œ ëª¨ë¸ì´ ì£¼ë¡œ ë³´ëŠ” ì˜ì—­ íŒ¨í„´ ë¶„ì„
        
        Args:
            dataloader: ë°ì´í„°ë¡œë”
            class_names: í´ë˜ìŠ¤ ì´ë¦„ ë¦¬ìŠ¤íŠ¸
            num_samples_per_class: í´ë˜ìŠ¤ë‹¹ ë¶„ì„í•  ìƒ˜í”Œ ìˆ˜
            
        Returns:
            dict: {class_name: [(x1, y1, x2, y2), ...]} í´ë˜ìŠ¤ë³„ ROI íŒ¨í„´
        """
        class_roi_patterns = {name: [] for name in class_names}
        class_sample_counts = {name: 0 for name in class_names}
        
        self.model.eval()
        with torch.no_grad():
            for images, labels in dataloader:
                for i, (image, label) in enumerate(zip(images, labels)):
                    class_name = class_names[label.item()]
                    
                    # ì´ë¯¸ ì¶©ë¶„í•œ ìƒ˜í”Œì„ ìˆ˜ì§‘í–ˆìœ¼ë©´ ìŠ¤í‚µ
                    if class_sample_counts[class_name] >= num_samples_per_class:
                        continue
                    
                    # Grad-CAM ìƒì„±
                    image_tensor = image.unsqueeze(0)
                    gradcam = self.generate_gradcam(image_tensor, label.item())
                    
                    # ROI ì¶”ì¶œ
                    roi_coords = self.get_roi_from_gradcam(gradcam)
                    class_roi_patterns[class_name].append(roi_coords)
                    class_sample_counts[class_name] += 1
                    
                    print(f"ğŸ“Š Analyzed {class_name}: {class_sample_counts[class_name]}/{num_samples_per_class}")
                
                # ëª¨ë“  í´ë˜ìŠ¤ì—ì„œ ì¶©ë¶„í•œ ìƒ˜í”Œì„ ìˆ˜ì§‘í–ˆìœ¼ë©´ ì¢…ë£Œ
                if all(count >= num_samples_per_class for count in class_sample_counts.values()):
                    break
        
        return class_roi_patterns
    
    def get_representative_roi_for_class(self, roi_patterns: List[Tuple], 
                                       method: str = 'median') -> Tuple[int, int, int, int]:
        """
        í´ë˜ìŠ¤ì˜ ëŒ€í‘œ ROI ì˜ì—­ ê³„ì‚°
        
        Args:
            roi_patterns: í•´ë‹¹ í´ë˜ìŠ¤ì˜ ROI íŒ¨í„´ ë¦¬ìŠ¤íŠ¸
            method: 'median', 'mean', 'most_common'
            
        Returns:
            tuple: (x1, y1, x2, y2) ëŒ€í‘œ ROI ì¢Œí‘œ
        """
        if not roi_patterns:
            return 0, 0, 100, 100  # ê¸°ë³¸ê°’
        
        if method == 'median':
            x1_values = [roi[0] for roi in roi_patterns]
            y1_values = [roi[1] for roi in roi_patterns]
            x2_values = [roi[2] for roi in roi_patterns]
            y2_values = [roi[3] for roi in roi_patterns]
            
            x1 = int(np.median(x1_values))
            y1 = int(np.median(y1_values))
            x2 = int(np.median(x2_values))
            y2 = int(np.median(y2_values))
            
            return x1, y1, x2, y2
        
        elif method == 'mean':
            x1 = int(np.mean([roi[0] for roi in roi_patterns]))
            y1 = int(np.mean([roi[1] for roi in roi_patterns]))
            x2 = int(np.mean([roi[2] for roi in roi_patterns]))
            y2 = int(np.mean([roi[3] for roi in roi_patterns]))
            
            return x1, y1, x2, y2
        
        # ì¶”ê°€ ë°©ë²•ë“¤ì€ í•„ìš”ì‹œ êµ¬í˜„
        return roi_patterns[0]  # ì²« ë²ˆì§¸ íŒ¨í„´ ë°˜í™˜


def visualize_gradcam(image: np.ndarray, gradcam: np.ndarray, 
                     roi_coords: Tuple[int, int, int, int] = None) -> np.ndarray:
    """
    Grad-CAM ì‹œê°í™”
    
    Args:
        image: ì›ë³¸ ì´ë¯¸ì§€ (H, W, 3)
        gradcam: Grad-CAM heatmap (H, W)
        roi_coords: ROI ì¢Œí‘œ (ì„ íƒì‚¬í•­)
        
    Returns:
        np.ndarray: ì‹œê°í™”ëœ ì´ë¯¸ì§€
    """
    # Grad-CAMì„ ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
    h, w = image.shape[:2]
    gradcam_resized = cv2.resize(gradcam, (w, h))
    
    # Heatmap ìƒì„±
    heatmap = cv2.applyColorMap(np.uint8(255 * gradcam_resized), cv2.COLORMAP_JET)
    
    # ì›ë³¸ ì´ë¯¸ì§€ì™€ ë¸”ë Œë“œ
    if len(image.shape) == 3:
        superimposed = heatmap * 0.4 + image * 0.6
    else:
        image_rgb = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)
        superimposed = heatmap * 0.4 + image_rgb * 0.6
    
    superimposed = np.uint8(superimposed)
    
    # ROI ë°•ìŠ¤ ê·¸ë¦¬ê¸°
    if roi_coords:
        x1, y1, x2, y2 = roi_coords
        cv2.rectangle(superimposed, (x1, y1), (x2, y2), (0, 255, 0), 2)
    
    return superimposed 