#!/usr/bin/env python3
"""
ğŸ¯ Wafer Defect Detection - Main Entry Point
ì§€ëŠ¥í˜• 2ë‹¨ê³„ ì›¨ì´í¼ ê²°í•¨ ê²€ì¶œ ì‹œìŠ¤í…œ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
"""

import os
import sys
import json
import argparse
import logging
from pathlib import Path
from typing import Dict, Any, List

from wafer_detector import WaferDetector, WaferDetectorError

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler('wafer_detection.log')
    ]
)
logger = logging.getLogger(__name__)

# ê¸°ë³¸ ì„¤ì •
DEFAULT_CONFIG = {
    'DATASET_ROOT': os.getenv('DATASET_ROOT', ''),  
    'MODEL_PATH': 'pretrained_models/convnextv2_base.fcmae_ft_in22k_in1k_pretrained.pth',
    'YOLO_MODEL': 'pretrained_models/yolo11x.pt',
    'CLASSIFICATION_SIZE': 384,
    'YOLO_SIZE': 1024,
    'PRECISION_THRESHOLD': 0.8,
    'CONFIDENCE_THRESHOLD': 0.7,
    'MAPPING_THRESHOLD': 0.3,
    'OUTPUT_DIR': 'outputs'
}


def load_config(config_path: Path) -> Dict[str, Any]:
    """
    ì„¤ì • íŒŒì¼ ë¡œë“œ
    
    Args:
        config_path: ì„¤ì • íŒŒì¼ ê²½ë¡œ
        
    Returns:
        ì„¤ì • ë”•ì…”ë„ˆë¦¬
    """
    if config_path.exists():
        try:
            with open(config_path, 'r') as f:
                user_config = json.load(f)
            
            # ê¸°ë³¸ ì„¤ì •ê³¼ ë³‘í•©
            config = DEFAULT_CONFIG.copy()
            config.update(user_config)
            
            logger.info(f"Configuration loaded from {config_path}")
            return config
        except Exception as e:
            logger.warning(f"Failed to load config: {e}, using defaults")
    
    return DEFAULT_CONFIG.copy()


def save_config(config: Dict[str, Any], config_path: Path) -> None:
    """
    ì„¤ì • íŒŒì¼ ì €ì¥
    
    Args:
        config: ì„¤ì • ë”•ì…”ë„ˆë¦¬
        config_path: ì €ì¥ ê²½ë¡œ
    """
    try:
        config_path.parent.mkdir(parents=True, exist_ok=True)
        with open(config_path, 'w') as f:
            json.dump(config, f, indent=2)
        logger.info(f"Configuration saved to {config_path}")
    except Exception as e:
        logger.warning(f"Failed to save config: {e}")


def validate_config(config: Dict[str, Any]) -> None:
    """
    ì„¤ì • ìœ íš¨ì„± ê²€ì¦
    
    Args:
        config: ì„¤ì • ë”•ì…”ë„ˆë¦¬
        
    Raises:
        ValueError: ì„¤ì •ì´ ìœ íš¨í•˜ì§€ ì•Šì€ ê²½ìš°
    """
    required_keys = ['MODEL_PATH', 'YOLO_MODEL', 'CLASSIFICATION_SIZE', 'YOLO_SIZE']
    for key in required_keys:
        if key not in config:
            raise ValueError(f"Missing required config key: {key}")
    
    # ëª¨ë¸ íŒŒì¼ ì¡´ì¬ í™•ì¸
    model_path = Path(config['MODEL_PATH'])
    yolo_path = Path(config['YOLO_MODEL'])
    
    if not model_path.exists():
        raise ValueError(f"Classification model not found: {model_path}")
    if not yolo_path.exists():
        raise ValueError(f"YOLO model not found: {yolo_path}")
    
    # ì„ê³„ê°’ ë²”ìœ„ í™•ì¸
    thresholds = ['PRECISION_THRESHOLD', 'CONFIDENCE_THRESHOLD', 'MAPPING_THRESHOLD']
    for key in thresholds:
        if key in config:
            value = config[key]
            if not 0 <= value <= 1:
                raise ValueError(f"{key} must be between 0 and 1, got {value}")


def run_full_pipeline(detector: WaferDetector, dataset_root: str, config: Dict[str, Any]) -> None:
    """
    ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (í•™ìŠµ ëª¨ë“œ)
    
    Args:
        detector: ì›¨ì´í¼ ê²€ì¶œê¸° ì¸ìŠ¤í„´ìŠ¤
        dataset_root: ë°ì´í„°ì…‹ ë£¨íŠ¸ ê²½ë¡œ
        config: ì„¤ì • ë”•ì…”ë„ˆë¦¬
    """
    try:
        logger.info("Starting full pipeline...")
        
        # Stage 1: ì„±ëŠ¥ ë¶„ì„
        logger.info("Stage 1: Performance Analysis")
        f1_scores = detector.analyze_performance(dataset_root)
        
        # Stage 2: ROI íŒ¨í„´ í•™ìŠµ
        logger.info("Stage 2: ROI Pattern Learning")
        detector.learn_roi_patterns(dataset_root)
        
        # Stage 3: í´ë˜ìŠ¤-ê°ì²´ ë§¤í•‘ ìƒì„±
        logger.info("Stage 3: Class-Object Mapping")
        detector.create_mapping(dataset_root)
        
        # Stage 4: ê²°ê³¼ ì €ì¥
        logger.info("Stage 4: Saving Results")
        detector.save_results(config['OUTPUT_DIR'])
        
        # í†µê³„ ì¶œë ¥
        stats = detector.get_stats()
        logger.info("Pipeline Statistics:")
        for key, value in stats.items():
            logger.info(f"   {key}: {value}")
        
        logger.info("Full pipeline completed successfully!")
        
    except Exception as e:
        logger.error(f"Pipeline failed: {str(e)}")
        raise


def run_prediction_mode(
    detector: WaferDetector, 
    predict_path: str, 
    config: Dict[str, Any]
) -> None:
    """
    ì˜ˆì¸¡ ëª¨ë“œ ì‹¤í–‰
    
    Args:
        detector: ì›¨ì´í¼ ê²€ì¶œê¸° ì¸ìŠ¤í„´ìŠ¤
        predict_path: ì˜ˆì¸¡ ëŒ€ìƒ ê²½ë¡œ
        config: ì„¤ì • ë”•ì…”ë„ˆë¦¬
    """
    try:
        predict_path = Path(predict_path)
        
        if not predict_path.exists():
            raise FileNotFoundError(f"Prediction path not found: {predict_path}")
        
        # ê¸°ì¡´ ê²°ê³¼ ë¡œë“œ ì‹œë„
        output_dir = Path(config['OUTPUT_DIR'])
        if output_dir.exists():
            try:
                detector.load_results(output_dir)
                logger.info("Loaded existing ROI patterns and mappings")
            except Exception as e:
                logger.warning(f"Failed to load existing results: {e}")
        
        if predict_path.is_file():
            # ë‹¨ì¼ ì´ë¯¸ì§€ ì˜ˆì¸¡
            logger.info(f"Predicting single image: {predict_path}")
            
            if not detector.classes:
                if not config['DATASET_ROOT']:
                    raise ValueError("DATASET_ROOT required for class loading")
                detector.load_classes(config['DATASET_ROOT'])
            
            result = detector.predict_image(str(predict_path))
            
            # ê²°ê³¼ ì¶œë ¥
            logger.info("Prediction Result:")
            logger.info(f"   Image: {result['image_path']}")
            logger.info(f"   Predicted Class: {result['predicted_class']}")
            logger.info(f"   Confidence: {result['confidence']:.3f}")
            logger.info(f"   Method: {result['method']}")
            
            if result['method'] == 'roi_enhanced':
                logger.info(f"   Detected Object: {result.get('detected_object', 'N/A')}")
                logger.info(f"   Object Counts: {result.get('object_counts', {})}")
            
        elif predict_path.is_dir():
            # í´ë” ì˜ˆì¸¡
            logger.info(f"Predicting folder: {predict_path}")
            
            # ì´ë¯¸ì§€ íŒŒì¼ ìˆ˜ì§‘
            image_files = []
            subdirs = [d for d in predict_path.iterdir() if d.is_dir()]
            
            if subdirs:
                # ImageFolder êµ¬ì¡°
                logger.info("Detected ImageFolder structure")
                detector.load_classes(str(predict_path))
                for subdir in subdirs:
                    image_files.extend(subdir.glob("*.jpg"))
                    image_files.extend(subdir.glob("*.png"))
                    image_files.extend(subdir.glob("*.jpeg"))
            else:
                # ë‹¨ìˆœ í´ë” êµ¬ì¡°
                logger.info("Detected simple folder structure")
                if not detector.classes:
                    if not config['DATASET_ROOT']:
                        raise ValueError("DATASET_ROOT required for class loading")
                    detector.load_classes(config['DATASET_ROOT'])
                
                image_files = list(predict_path.glob("*.jpg"))
                image_files.extend(predict_path.glob("*.png"))
                image_files.extend(predict_path.glob("*.jpeg"))
            
            if not image_files:
                logger.warning("No image files found")
                return
            
            logger.info(f"Found {len(image_files)} images to process")
            
            # ë°°ì¹˜ ì˜ˆì¸¡
            results = []
            method_counts = {'classification_only': 0, 'roi_enhanced': 0}
            
            for i, img_path in enumerate(image_files):
                try:
                    result = detector.predict_image(str(img_path))
                    results.append(result)
                    method_counts[result['method']] += 1
                    
                    if (i + 1) % 10 == 0:
                        logger.info(f"Processed {i + 1}/{len(image_files)} images")
                    
                except Exception as e:
                    logger.warning(f"Failed to predict {img_path}: {e}")
                    continue
            
            # ê²°ê³¼ ì €ì¥
            output_dir = Path(config['OUTPUT_DIR'])
            output_dir.mkdir(parents=True, exist_ok=True)
            
            results_file = output_dir / 'prediction_results.json'
            with open(results_file, 'w') as f:
                json.dump(results, f, indent=2)
            
            # í†µê³„ ì¶œë ¥
            logger.info("Batch Prediction Results:")
            logger.info(f"   Total Images: {len(image_files)}")
            logger.info(f"   Successful Predictions: {len(results)}")
            logger.info(f"   Classification Only: {method_counts['classification_only']}")
            logger.info(f"   ROI Enhanced: {method_counts['roi_enhanced']}")
            logger.info(f"   Results saved to: {results_file}")
            
            # í´ë˜ìŠ¤ë³„ ì˜ˆì¸¡ ë¶„í¬
            class_counts = {}
            for result in results:
                class_name = result['predicted_class']
                class_counts[class_name] = class_counts.get(class_name, 0) + 1
            
            logger.info("   Class Distribution:")
            for class_name, count in sorted(class_counts.items()):
                logger.info(f"     {class_name}: {count}")
        
        else:
            raise ValueError(f"Invalid prediction path: {predict_path}")
            
    except Exception as e:
        logger.error(f"Prediction failed: {str(e)}")
        raise


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(
        description="Wafer Defect Detection with ROI Enhancement",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
    # Full pipeline (training mode)
    python main.py /path/to/dataset
    
    # Predict single image
    python main.py --predict /path/to/image.jpg
    
    # Predict folder
    python main.py --predict /path/to/images/
    
    # Custom config and output
    python main.py /path/to/dataset --config config.json --output-dir results/
        """
    )
    
    parser.add_argument(
        "dataset_path", 
        nargs='?', 
        help="Dataset root path (for training mode)"
    )
    parser.add_argument(
        "--predict", 
        help="Predict single image or folder"
    )
    parser.add_argument(
        "--config", 
        default="config.json",
        help="Configuration file path (default: config.json)"
    )
    parser.add_argument(
        "--output-dir", 
        help="Output directory (overrides config)"
    )
    parser.add_argument(
        "--load-results",
        action="store_true",
        help="Load existing results before prediction"
    )
    parser.add_argument(
        "--verbose", 
        action="store_true",
        help="Enable verbose logging"
    )
    
    args = parser.parse_args()
    
    # ë¡œê¹… ë ˆë²¨ ì¡°ì •
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    try:
        # ì„¤ì • ë¡œë“œ
        config_path = Path(args.config)
        config = load_config(config_path)
        
        # ëª…ë ¹í–‰ ì¸ìˆ˜ë¡œ ì„¤ì • ì˜¤ë²„ë¼ì´ë“œ
        if args.dataset_path:
            config['DATASET_ROOT'] = args.dataset_path
        if args.output_dir:
            config['OUTPUT_DIR'] = args.output_dir
        
        # ì„¤ì • ìœ íš¨ì„± ê²€ì¦
        validate_config(config)
        
        # ì„¤ì • ì €ì¥ (ì—…ë°ì´íŠ¸ëœ ê²½ìš°)
        save_config(config, config_path)
        
        logger.info("Starting Wafer Defect Detection System")
        logger.info(f"Configuration: {config}")
        
        # ê²€ì¶œê¸° ì´ˆê¸°í™”
        detector = WaferDetector(config)
        detector.load_models(config['MODEL_PATH'], config['YOLO_MODEL'])
        
        # ì‹¤í–‰ ëª¨ë“œ ê²°ì •
        if args.predict:
            # ì˜ˆì¸¡ ëª¨ë“œ
            run_prediction_mode(detector, args.predict, config)
        else:
            # í•™ìŠµ ëª¨ë“œ
            if not config['DATASET_ROOT']:
                raise ValueError("Dataset path required for training mode. "
                               "Provide as argument or set DATASET_ROOT in config.")
            run_full_pipeline(detector, config['DATASET_ROOT'], config)
        
        logger.info("Process completed successfully!")
        
    except KeyboardInterrupt:
        logger.info("Process interrupted by user")
        sys.exit(1)
    except Exception as e:
        logger.error(f"Process failed: {str(e)}")
        sys.exit(1)


if __name__ == "__main__":
    main()
